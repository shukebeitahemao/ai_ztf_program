import os
import time
import pandas as pd
from bs4 import BeautifulSoup as bs  # HTMLè§£æåº“ï¼Œç”¨äºä»ç½‘é¡µä¸­æå–æ•°æ®
from get_url_text import get_url_text  # è‡ªå®šä¹‰çš„ç½‘ç»œè¯·æ±‚æ¨¡å—
import random
#åˆ©ç”¨playwrigrtè·å–ç™»é™†cookie


# å¤„ç†çƒ­æ¦œæ•°







#è¿™éƒ¨åˆ†ä»£ç ç”¨äºæ ¹æ®é—®é¢˜åˆ—è¡¨çˆ¬å–é—®é¢˜çš„å…ƒä¿¡æ¯ã€‚
# å¯ä»é—®é¢˜/ä¸“æ åˆ—è¡¨ä¸­è¯»å–å¹¶ç­›é€‰å‡ºç¬¦åˆæ¡ä»¶ï¼ˆå¦‚é â€œä¸“æ â€ ç±»å‹ä¸”åœ¨æŒ‡å®šæ—¥æœŸä¹‹åï¼‰çš„é—®é¢˜ ID åˆ—è¡¨ï¼Œ
# å¹¶çˆ¬å–è¿™äº›é—®é¢˜çš„å…ƒæ•°æ®ã€‚
def get_question_list(filename):
    """
    ğŸ“– è¯»å–é—®é¢˜åˆ—è¡¨å¹¶è¿›è¡Œç­›é€‰
    
    å‚æ•°ï¼š
        filename: é—®é¢˜åˆ—è¡¨CSVæ–‡ä»¶è·¯å¾„ï¼ˆé€šå¸¸æ˜¯question_list.csvï¼‰
    
    è¿”å›ï¼š
        q_list: ç­›é€‰åçš„é—®é¢˜åˆ—è¡¨
    
    ç­›é€‰æ¡ä»¶ï¼š
        1. æ’é™¤ä¸“æ æ–‡ç« ï¼ˆtype != "ä¸“æ "ï¼‰
        2. åªä¿ç•™æŒ‡å®šæ—¥æœŸåçš„é—®é¢˜ï¼ˆå¯æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
    """
    df = pd.read_csv(filename)
    
    # ç­›é€‰æ¡ä»¶1ï¼šæ’é™¤ä¸“æ æ–‡ç« ï¼Œåªä¿ç•™é—®é¢˜å’Œå›ç­”
    df = df[df["type"] != "ä¸“æ "]
    
    # ç­›é€‰æ¡ä»¶2ï¼šåªçˆ¬å–æŒ‡å®šæ—¥æœŸä¹‹åçš„é—®é¢˜ï¼ˆå¯æ ¹æ®éœ€è¦ä¿®æ”¹ï¼‰
    df = df[df["date"] >= "2024-10-14"]
    
    # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼ï¼Œæ–¹ä¾¿åç»­å¤„ç†
    q_list = df.values.tolist()
    return q_list

def get_question_data(html_text):
    """
    ğŸ” ä»HTMLé¡µé¢ä¸­è§£æé—®é¢˜çš„å…ƒæ•°æ®
    
    å‚æ•°ï¼š
        html_text: é—®é¢˜è¯¦æƒ…é¡µçš„HTMLæºç 
    
    è¿”å›ï¼š
        list: åŒ…å«é—®é¢˜å„é¡¹å…ƒæ•°æ®çš„åˆ—è¡¨
              [é—®é¢˜ID, é—®é¢˜æ ‡é¢˜, å…³æ³¨æ•°, æµè§ˆæ•°, å›ç­”æ•°, è¯é¢˜æ ‡ç­¾, åˆ›å»ºæ—¥æœŸ]
    
    è§£æçš„å…ƒæ•°æ®åŒ…æ‹¬ï¼š
        - qContent: é—®é¢˜æ ‡é¢˜
        - followerCount: å…³æ³¨è¯¥é—®é¢˜çš„äººæ•°
        - viewCount: é—®é¢˜æµè§ˆé‡
        - answerCount: å›ç­”æ•°é‡
        - topicTag: é—®é¢˜æ‰€å±çš„è¯é¢˜æ ‡ç­¾
        - date: é—®é¢˜åˆ›å»ºæ—¥æœŸ
    """

    try:
        bsobj = bs(html_text, "html.parser")

        qContent = bsobj.find_all("meta", attrs={"itemprop": "name"})[0]["content"]
        followerCount = bsobj.find_all("strong", attrs={"class": "NumberBoard-itemValue"})[0]["title"]
        viewCount = bsobj.find_all("strong", attrs={"class": "NumberBoard-itemValue"})[1]["title"]
        answerCount = bsobj.find_all("meta", attrs={"itemprop": "answerCount"})[0]["content"]
        topicTag = bsobj.find_all("meta", attrs={"itemprop": "keywords"})[0]["content"]
        date = bsobj.find_all("meta", attrs={"itemprop": "dateCreated"})[0]["content"]

        return [q_id, qContent, followerCount, viewCount, answerCount, topicTag, date[:10]]

    except:
        print("Unknown Error !")
        return [
            q_id,
            "UnknownError",
            "UnknownError",
            "UnknownError",
            "UnknownError",
            "UnknownError",
            "UnknownError",
        ]

def save_data(q_info_list, filename):
    """
    ğŸ’¾ ä¿å­˜é—®é¢˜å…ƒæ•°æ®åˆ°CSVæ–‡ä»¶
    
    å‚æ•°ï¼š
        q_info_list: é—®é¢˜ä¿¡æ¯åˆ—è¡¨
        filename: è¾“å‡ºæ–‡ä»¶åï¼ˆé€šå¸¸æ˜¯question_meta_info.csvï¼‰
    
    æ•°æ®å¤„ç†é€»è¾‘ï¼š
        1. åˆ›å»ºDataFrameå¹¶è®¾ç½®åˆ—å
        2. å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆ™åˆå¹¶æ–°æ—§æ•°æ®
        3. æ¸…ç†æ— æ•ˆæ•°æ®ï¼ˆUnknownErrorï¼‰
        4. æŒ‰é—®é¢˜IDå»é‡ï¼Œä¿ç•™æœ€æ–°æ•°æ®
        5. æ ¼å¼åŒ–æ—¥æœŸå¹¶æ’åº
        6. ä¿å­˜åˆ°CSVæ–‡ä»¶
    """
    # åˆ›å»ºDataFrameï¼Œè®¾ç½®åˆ—å
    df = pd.DataFrame(
        q_info_list,
        columns=[
            "q_id",           # é—®é¢˜ID
            "q_content",      # é—®é¢˜æ ‡é¢˜
            "followerCount",  # å…³æ³¨æ•°
            "viewCount",      # æµè§ˆæ•°
            "answerCount",    # å›ç­”æ•°
            "topicTag",       # è¯é¢˜æ ‡ç­¾
            "created_date"    # åˆ›å»ºæ—¥æœŸ
        ],
    )
    
    # å¦‚æœæ–‡ä»¶å·²ç»å­˜åœ¨ï¼Œåˆå¹¶æ–°æ—§æ•°æ®
    if os.path.exists(filename):
        df_old = pd.read_csv(filename)
        df = pd.concat([df_old, df], ignore_index=True)
        
        # æ•°æ®æ¸…ç†ï¼šåˆ é™¤è§£æå¤±è´¥çš„é—®é¢˜ï¼ˆæ ‡è®°ä¸ºUnknownErrorçš„è®°å½•ï¼‰
        df = df[df["q_content"] != "UnknownError"]
        
        # æŒ‰é—®é¢˜IDå»é‡ï¼Œä¿ç•™æœ€æ–°çš„æ•°æ®
        df = df.drop_duplicates(subset=["q_id"], keep="last")
        
        # æ—¥æœŸæ ¼å¼åŒ–ï¼šå°†"-"æ›¿æ¢ä¸º"/"
        df["created_date"] = df["created_date"].str.replace("-", "/")
        df["created_date"] = pd.to_datetime(df["created_date"])
        
        # æŒ‰åˆ›å»ºæ—¥æœŸæ’åº
        df = df.sort_values(by=["created_date"])
        
        # ä¿å­˜åˆ°CSVæ–‡ä»¶ï¼Œä½¿ç”¨UTF-8ç¼–ç ä»¥æ”¯æŒä¸­æ–‡
        df.to_csv(filename, index=False, header=True, encoding="utf-8")
#TODO æŒ‡å®šé—®é¢˜åˆ—è¡¨
q_list = get_question_list("data/question_list.csv")
print(f"å…±{len(q_list)}ä¸ªé—®é¢˜")
q_info_list = []

#TODO å¯è®¾ç½®å¼€å§‹å’Œç»“æŸä½ç½®ï¼Œç”¨äºåœ¨å‡ºé”™ä¸­æ–­æ—¶é‡æ–°çˆ¬å–
for i, item in enumerate(q_list[:]):
    #å®é™…ä¸Šåªç”¨åˆ°äº†idåˆ—ï¼Œå…¶ä»–åˆ—å¯ä»¥å¿½ç•¥
    q_id = item[1]

    url = f"https://www.zhihu.com/question/{str(q_id)}"
    text = get_url_text(url)
    q_info = get_question_data(text)
    q_info_list.append(q_info)

    if i % 30 == 0:
        print(q_info[1])
        save_data(q_info_list, "data/question_meta_info.csv")
        q_info_list = []
        time.sleep(random.uniform(0, 2))
        print(f"å·²ä¿å­˜{i+1}æ¡æ•°æ®")

save_data(q_info_list, "data/question_meta_info.csv")

