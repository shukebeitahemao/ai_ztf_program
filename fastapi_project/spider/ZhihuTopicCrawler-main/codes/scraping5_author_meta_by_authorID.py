"""
çŸ¥ä¹çˆ¬è™« - ç¬¬äº”æ­¥ï¼šæ ¹æ®ç”¨æˆ·IDè·å–ä½œè€…è¯¦ç»†ä¿¡æ¯
=========================================

ğŸ¯ åŠŸèƒ½è¯´æ˜ï¼š
- è¾“å…¥ï¼šç”¨æˆ·tokenåˆ—è¡¨ï¼ˆç”±scraping4.5ç”Ÿæˆï¼‰
- è¾“å‡ºï¼šæ¯ä¸ªç”¨æˆ·çš„è¯¦ç»†ä¿¡æ¯ï¼ˆç²‰ä¸æ•°ã€è·èµæ•°ã€å¾½ç« ç­‰ï¼‰
- ç”¨é€”ï¼šè·å–å›ç­”è€…çš„è¯¦ç»†èµ„æ–™ï¼Œç”¨äºç”¨æˆ·ç”»åƒåˆ†æ

ğŸ“Š è·å–çš„ç”¨æˆ·ä¿¡æ¯ï¼š
- åŸºæœ¬ä¿¡æ¯ï¼šç”¨æˆ·åã€æ€§åˆ«ã€IPåœ°å€
- ç»Ÿè®¡æ•°æ®ï¼šè·èµæ•°ã€è¢«æ„Ÿè°¢æ•°ã€ç²‰ä¸æ•°ã€è¢«æ”¶è—æ•°
- åˆ›ä½œæ•°æ®ï¼šå›ç­”æ•°+æ–‡ç« æ•°
- è®¤è¯ä¿¡æ¯ï¼šVIPçŠ¶æ€ã€èº«ä»½è®¤è¯ã€ä¼˜ç§€å›ç­”è€…ç­‰å¾½ç« 

ğŸš€ æ•°æ®æ¥æºï¼š
- ç”¨æˆ·ä¸»é¡µHTMLä¸­çš„JavaScriptåˆå§‹åŒ–æ•°æ®
- è§£æJSONæ ¼å¼çš„ç”¨æˆ·å®Œæ•´ä¿¡æ¯

âš ï¸ æ³¨æ„äº‹é¡¹ï¼š
- éœ€è¦å…ˆè¿è¡Œ scraping4.5 ç”Ÿæˆç”¨æˆ·tokenåˆ—è¡¨
- å®¹æ˜“è§¦å‘éªŒè¯ç ï¼Œéœ€è¦å®šæœŸæ›´æ–°cookie
- è¿ç»­å¤±è´¥5æ¬¡ä¼šè‡ªåŠ¨åœæ­¢
- æ”¯æŒæ–­ç‚¹ç»­çˆ¬ï¼ˆè‡ªåŠ¨è·³è¿‡å·²çˆ¬å–çš„ç”¨æˆ·ï¼‰

ä¸Šæ¬¡è¿è¡Œï¼š2024/11/16 15:05
"""

import os
import time
import json
import pandas as pd
from bs4 import BeautifulSoup as bs
from get_url_text import get_url_text

def get_tokens(sourse_filename, data_store_file):
    # sourse_filename: scraping3.5_data_processing.py output
    # data_store_file: output file name
    # Import csv, convert to list
    df = pd.read_csv(sourse_filename, header=None)
    token_list = df.iloc[:, 0].tolist()
    print(f"Total {len(token_list)} users")

    if not os.path.exists(data_store_file):
        a = pd.DataFrame(
            [],
            columns=[
                "user_token",
                "name",
                "gender",
                "IP_address",
                "voteupCount",
                "thankedCount",
                "followerCount",
                "favoritedCount",
                "productCount",
                "VIPs",
                "identity",
                "top_writer",
            ],
        )
        a.to_csv(data_store_file, index=False, header=True)
        print(f"Create new file: {data_store_file}")
    else:
        df_exist = pd.read_csv(data_store_file)
        token_exist = df_exist["user_token"].tolist()
        token_list = list(set(token_list) - set(token_exist))

    print(f"Find {len(token_list)} new users")
    return token_list

def get_author_info(user_text, token):
    try:
        json_text = bs(user_text, "html.parser").find("script", attrs={"id": "js-initialData"}).text
        json_data = json.loads(json_text)["initialState"]["entities"]["users"][token]

        token = json_data["urlToken"]
        name = json_data["name"]
        gender = json_data["gender"]
        IP_address = json_data["ipInfo"][5:]
        voteupCount = json_data["voteupCount"]
        thankedCount = json_data["thankedCount"]
        followerCount = json_data["followerCount"]
        favoritedCount = json_data["favoritedCount"]
        productCount = json_data["answerCount"] + json_data["articlesCount"]
        VIPs = json_data["vipInfo"]["isVip"] + json_data["kvipInfo"]["isVip"]
        badge_info = json_data["badgeV2"]["mergedBadges"]
        identity = 1 if "identity" in [badge["type"] for badge in badge_info] else 0
        top_writer = 1 if "best" in [badge["type"] for badge in badge_info] else 0

        return [
            token, # ç”¨æˆ·token
            name, # ç”¨æˆ·å
            gender, # æ€§åˆ«
            IP_address, # IPåœ°å€
            voteupCount, # æ€»è·èµæ•°
            thankedCount, # è¢«å–œæ¬¢æ•°
            followerCount, # ç²‰ä¸æ•°
            favoritedCount, # è¢«æ”¶è—æ•°
            productCount, # å›ç­”æ•°+æ–‡ç« æ•°
            VIPs, # æ‹¥æœ‰å‡ é¡¹VIP
            identity, # æ˜¯å¦æ‹¥æœ‰identityå¾½ç« 
            top_writer, # æ˜¯å¦æ‹¥æœ‰bestå¾½ç« 
        ]
    except:
        print(f"{token} Text Error !")
        return None

def save_data(user_info_list, filename):
    df = pd.DataFrame(
        user_info_list,
        columns=[
            "user_token",
            "name",
            "gender",
            "IP_address",
            "voteupCount",
            "thankedCount",
            "followerCount",
            "favoritedCount",
            "productCount",
            "VIPs",
            "identity",
            "top_writer",
        ],
    )

    df.to_csv(filename, index=False, mode="a", header=False)

# RUN `scraping3.5_data_processing.py` FIRST!
# çŸ¥ä¹ä¹±ç ä¸ä¼šå½±å“è·å–ç”¨æˆ·ä¿¡æ¯

if __name__ == "__main__":
    #TODO: è¾“å…¥æ–‡ä»¶å
    token_list = get_tokens(
        sourse_filename="data/user_tokens.csv", 
        data_store_file="data/author_meta_info.csv"
    )
    user_info_list = []

    error_num = 0
    for i, token in enumerate(token_list):
        if token:
            url = f"https://www.zhihu.com/people/{str(token)}"
            user_text = get_url_text(url)

            if user_text and ("è¯¥è´¦å·å·²" in user_text or "è¯¥ç”¨æˆ·å·²" in user_text):
                user_info = [token] + ["None"] * 11
                user_info_list.append(user_info)
                save_data(user_info_list, "data/author_meta_info.csv")
                user_info_list = []
                print(f"âš ï¸âš ï¸âš ï¸{token}å·²è¢«å°ç¦âš ï¸âš ï¸âš ï¸")
                continue

            user_info = get_author_info(user_text, token)

            if user_info:
                error_num = 0
                user_info_list.append(user_info)
            else:
                error_num += 1  # åˆ¤æ–­è¿ç»­é”™è¯¯ï¼Œè¾¾åˆ°5ä¸ªæ—¶è®¤ä¸ºå‡ºç°éªŒè¯ç é”™è¯¯

        if error_num >= 5:
            print(f"âš ï¸âš ï¸âš ï¸éœ€è¦å¡«å†™éªŒè¯ç å¹¶é‡æ–°è¿è¡Œâš ï¸âš ï¸âš ï¸")
            break

        if i % 30 == 0:
            time.sleep(0.5)
            save_data(user_info_list, "data/author_meta_info.csv")
            user_info_list = []

    save_data(user_info_list, "data/author_meta_info.csv")

    print("Finish!")
