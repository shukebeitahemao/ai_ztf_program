#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çŸ¥ä¹HTMLé¡µé¢è§£æè„šæœ¬
åŠŸèƒ½ï¼šä½¿ç”¨get_url_textè·å–ç½‘é¡µHTMLå†…å®¹ï¼Œè§£æçƒ­é—¨é—®é¢˜ä¿¡æ¯ï¼Œæå–titleã€idã€urlã€abstractã€hotç­‰ä¿¡æ¯å¹¶ä¿å­˜åˆ°CSVæ–‡ä»¶
ä½œè€…ï¼šAIåŠ©æ‰‹
åˆ›å»ºæ—¶é—´ï¼š2025-01-20
ä¿®æ”¹æ—¶é—´ï¼š2025-01-20
"""

import os
import re
import csv
import pandas as pd
from datetime import datetime
from bs4 import BeautifulSoup
from get_url_text import get_url_text

def parse_html_from_url(url):
    """
    ä»URLè·å–HTMLå†…å®¹å¹¶æå–çƒ­é—¨é—®é¢˜ä¿¡æ¯
    
    Args:
        url (str): è¦è§£æçš„ç½‘é¡µURL
        
    Returns:
        list: åŒ…å«è§£æç»“æœçš„å­—å…¸åˆ—è¡¨
    """
    print(f"ğŸŒ å¼€å§‹è·å–ç½‘é¡µå†…å®¹: {url}")
    
    # ä½¿ç”¨get_url_textè·å–ç½‘é¡µå†…å®¹
    try:
        html_content = get_url_text(url)
        if not html_content:
            print("âŒ è·å–ç½‘é¡µå†…å®¹å¤±è´¥ï¼šè¿”å›å†…å®¹ä¸ºç©º")
            return []
        print(f"âœ… æˆåŠŸè·å–ç½‘é¡µå†…å®¹ï¼Œé•¿åº¦: {len(html_content)} å­—ç¬¦")
    except Exception as e:
        print(f"âŒ è·å–ç½‘é¡µå†…å®¹å¤±è´¥: {e}")
        return []
    
    # ä½¿ç”¨BeautifulSoupè§£æHTML
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        print("âœ… HTMLè§£ææˆåŠŸ")
    except Exception as e:
        print(f"âŒ HTMLè§£æå¤±è´¥: {e}")
        return []
    
    # æŸ¥æ‰¾æ‰€æœ‰çš„sectionå…ƒç´ 
    sections = soup.find_all('section', class_='HotItem')
    print(f"ğŸ“Š æ‰¾åˆ° {len(sections)} ä¸ªçƒ­é—¨é—®é¢˜section")
    
    if len(sections) == 0:
        print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•çƒ­é—¨é—®é¢˜sectionï¼Œå¯èƒ½ç½‘é¡µç»“æ„å·²å˜åŒ–")
        # å°è¯•æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„ç»“æ„
        print("ğŸ” å°è¯•æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„é—®é¢˜å®¹å™¨...")
        alternative_sections = soup.find_all(['div', 'article'], class_=lambda x: x and ('item' in x.lower() or 'question' in x.lower()))
        if alternative_sections:
            print(f"ğŸ“Š æ‰¾åˆ° {len(alternative_sections)} ä¸ªå¯èƒ½çš„é—®é¢˜å®¹å™¨")
            sections = alternative_sections
        else:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•é—®é¢˜ç›¸å…³çš„å®¹å™¨")
    
    results = []
    current_date = datetime.now().strftime("%Y-%m-%d")
    
    for i, section in enumerate(sections, 1):
        try:
            print(f"\nğŸ” æ­£åœ¨è§£æç¬¬ {i} ä¸ªsection...")
            
            # æå–title - å°è¯•å¤šç§å¯èƒ½çš„é€‰æ‹©å™¨
            title_element = section.find('h2', class_='HotItem-title')
            if not title_element:
                # å°è¯•å…¶ä»–å¯èƒ½çš„æ ‡é¢˜é€‰æ‹©å™¨
                title_element = section.find(['h1', 'h2', 'h3'], class_=lambda x: x and ('title' in x.lower()))
                if not title_element:
                    title_element = section.find(['h1', 'h2', 'h3'])
            
            title = title_element.get_text(strip=True) if title_element else ""
            print(f"ğŸ“ æ ‡é¢˜: {title[:50]}...")
            
            # æå–URLå’ŒID - ä»aæ ‡ç­¾çš„hrefå±æ€§ä¸­è·å–
            link_element = section.find('a', href=True)
            if not link_element:
                # å°è¯•æŸ¥æ‰¾ä»»ä½•åŒ…å«hrefçš„é“¾æ¥
                link_element = section.find('a')
            
            url_link = ""
            question_id = ""
            
            if link_element and link_element.get('href'):
                url_link = link_element['href']
                # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè¡¥å……å®Œæ•´URL
                if url_link.startswith('/'):
                    url_link = f"https://www.zhihu.com{url_link}"
                
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–é—®é¢˜ID
                match = re.search(r'/question/(\d+)', str(url_link))
                if match:
                    question_id = match.group(1)
                print(f"ğŸ”— URL: {url_link}")
                print(f"ğŸ†” ID: {question_id}")
            
            # æå–abstract - å°è¯•å¤šç§å¯èƒ½çš„é€‰æ‹©å™¨
            excerpt_element = section.find('p', class_='HotItem-excerpt')
            if not excerpt_element:
                # å°è¯•å…¶ä»–å¯èƒ½çš„æ‘˜è¦é€‰æ‹©å™¨
                excerpt_element = section.find('p', class_=lambda x: x and ('excerpt' in x.lower() or 'summary' in x.lower()))
                if not excerpt_element:
                    excerpt_element = section.find('p')
            
            abstract = excerpt_element.get_text(strip=True) if excerpt_element else ""
            # æ¸…ç†abstractæ–‡æœ¬ï¼Œå»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
            abstract = re.sub(r'\s+', ' ', abstract)
            print(f"ğŸ“„ æ‘˜è¦: {abstract[:50]}...")
            
            # æå–çƒ­åº¦ä¿¡æ¯ - ä»åŒ…å«"çƒ­åº¦"çš„æ–‡æœ¬ä¸­æå–
            hot = ""
            # æŸ¥æ‰¾åŒ…å«çƒ­åº¦ä¿¡æ¯çš„æ–‡æœ¬
            hot_text = section.get_text()
            hot_match = re.search(r'(\d+(?:\.\d+)?)\s*ä¸‡?\s*çƒ­åº¦', hot_text)
            if hot_match:
                hot_value = hot_match.group(1)
                if 'ä¸‡' in hot_match.group(0):
                    hot = f"{hot_value}ä¸‡çƒ­åº¦"
                else:
                    hot = f"{hot_value}çƒ­åº¦"
            else:
                # å°è¯•å…¶ä»–çƒ­åº¦æ ¼å¼
                hot_match2 = re.search(r'(\d+(?:\.\d+)?)\s*ä¸‡?\s*(å…³æ³¨|æµè§ˆ|é˜…è¯»)', hot_text)
                if hot_match2:
                    hot_value = hot_match2.group(1)
                    hot_type = hot_match2.group(2)
                    if 'ä¸‡' in hot_match2.group(0):
                        hot = f"{hot_value}ä¸‡{hot_type}"
                    else:
                        hot = f"{hot_value}{hot_type}"
            
            print(f"ğŸ”¥ çƒ­åº¦: {hot}")
            
            # åªæœ‰å½“è·å–åˆ°åŸºæœ¬ä¿¡æ¯æ—¶æ‰æ·»åŠ åˆ°ç»“æœä¸­
            if title or question_id:
                # æ„å»ºç»“æœå­—å…¸
                result = {
                    'type': 'çƒ­ç‚¹é—®é¢˜',
                    'id': question_id,
                    'title': title,
                    'url': url_link,
                    'date': current_date,
                    'hot': hot,
                    'abstract': abstract
                }
                
                results.append(result)
                print(f"âœ… ç¬¬ {i} ä¸ªsectionè§£æå®Œæˆ")
            else:
                print(f"âš ï¸ ç¬¬ {i} ä¸ªsectionç¼ºå°‘å…³é”®ä¿¡æ¯ï¼Œè·³è¿‡")
            
        except Exception as e:
            print(f"âŒ è§£æç¬¬ {i} ä¸ªsectionæ—¶å‡ºé”™: {e}")
            continue
    
    print(f"\nğŸ“Š æ€»å…±æˆåŠŸè§£æ {len(results)} æ¡è®°å½•")
    return results

def parse_html_file(html_file_path):
    """
    è§£ææœ¬åœ°HTMLæ–‡ä»¶ï¼ˆä¿ç•™åŸåŠŸèƒ½ä½œä¸ºå¤‡ç”¨ï¼‰
    
    Args:
        html_file_path (str): HTMLæ–‡ä»¶è·¯å¾„
        
    Returns:
        list: åŒ…å«è§£æç»“æœçš„å­—å…¸åˆ—è¡¨
    """
    print(f"ğŸ“– å¼€å§‹è§£ææœ¬åœ°HTMLæ–‡ä»¶: {html_file_path}")
    
    # è¯»å–HTMLæ–‡ä»¶å†…å®¹
    try:
        with open(html_file_path, 'r', encoding='utf-8') as f:
            html_content = f.read()
        print(f"âœ… æˆåŠŸè¯»å–HTMLæ–‡ä»¶")
    except Exception as e:
        print(f"âŒ è¯»å–HTMLæ–‡ä»¶å¤±è´¥: {e}")
        return []
    
    # ä½¿ç”¨BeautifulSoupè§£æHTML
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # æŸ¥æ‰¾æ‰€æœ‰çš„sectionå…ƒç´ 
    sections = soup.find_all('section', class_='HotItem')
    print(f"ğŸ“Š æ‰¾åˆ° {len(sections)} ä¸ªçƒ­é—¨é—®é¢˜section")
    
    results = []
    current_date = datetime.now().strftime("%Y-%m-%d")
    
    for i, section in enumerate(sections, 1):
        try:
            print(f"\nğŸ” æ­£åœ¨è§£æç¬¬ {i} ä¸ªsection...")
            
            # æå–title - ä»h2æ ‡ç­¾ä¸­è·å–
            title_element = section.find('h2', class_='HotItem-title')
            title = title_element.get_text(strip=True) if title_element else ""
            print(f"ğŸ“ æ ‡é¢˜: {title[:50]}...")
            
            # æå–URLå’ŒID - ä»aæ ‡ç­¾çš„hrefå±æ€§ä¸­è·å–
            link_element = section.find('a', href=True)
            url = ""
            question_id = ""
            
            if link_element and link_element.get('href'):
                url = link_element['href']
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–é—®é¢˜ID
                match = re.search(r'/question/(\d+)', url)
                if match:
                    question_id = match.group(1)
                print(f"ğŸ”— URL: {url}")
                print(f"ğŸ†” ID: {question_id}")
            
            # æå–abstract - ä»pæ ‡ç­¾ä¸­è·å–
            excerpt_element = section.find('p', class_='HotItem-excerpt')
            abstract = excerpt_element.get_text(strip=True) if excerpt_element else ""
            # æ¸…ç†abstractæ–‡æœ¬ï¼Œå»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
            abstract = re.sub(r'\s+', ' ', abstract)
            print(f"ğŸ“„ æ‘˜è¦: {abstract[:50]}...")
            
            # æå–çƒ­åº¦ä¿¡æ¯ - ä»åŒ…å«"çƒ­åº¦"çš„æ–‡æœ¬ä¸­æå–
            hot = ""
            # æŸ¥æ‰¾åŒ…å«çƒ­åº¦ä¿¡æ¯çš„æ–‡æœ¬
            hot_text = section.get_text()
            hot_match = re.search(r'(\d+(?:\.\d+)?)\s*ä¸‡?\s*çƒ­åº¦', hot_text)
            if hot_match:
                hot_value = hot_match.group(1)
                if 'ä¸‡' in hot_match.group(0):
                    hot = f"{hot_value}ä¸‡çƒ­åº¦"
                else:
                    hot = f"{hot_value}çƒ­åº¦"
            print(f"ğŸ”¥ çƒ­åº¦: {hot}")
            
            # æ„å»ºç»“æœå­—å…¸
            result = {
                'type': 'çƒ­ç‚¹é—®é¢˜',
                'id': question_id,
                'title': title,
                'url': url,
                'date': current_date,
                'hot': hot,
                'abstract': abstract
            }
            
            results.append(result)
            print(f"âœ… ç¬¬ {i} ä¸ªsectionè§£æå®Œæˆ")
            
        except Exception as e:
            print(f"âŒ è§£æç¬¬ {i} ä¸ªsectionæ—¶å‡ºé”™: {e}")
            continue
    
    print(f"\nğŸ“Š æ€»å…±æˆåŠŸè§£æ {len(results)} æ¡è®°å½•")
    return results

def save_to_csv(data, output_file):
    """
    å°†æ•°æ®ä¿å­˜åˆ°CSVæ–‡ä»¶
    
    Args:
        data (list): è¦ä¿å­˜çš„æ•°æ®åˆ—è¡¨
        output_file (str): è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    if not data:
        print("âŒ æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
        return
    
    print(f"ğŸ’¾ å¼€å§‹ä¿å­˜æ•°æ®åˆ°: {output_file}")
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    output_dir = os.path.dirname(output_file)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
        print(f"ğŸ“ åˆ›å»ºç›®å½•: {output_dir}")
    
    try:
        # ä½¿ç”¨pandasä¿å­˜æ•°æ®
        df = pd.DataFrame(data)
        
        # é‡æ–°æ’åˆ—åˆ—çš„é¡ºåº
        columns_order = ['type', 'id', 'title', 'url', 'date', 'hot', 'abstract']
        #å…ˆå–å¾—å‰5åˆ—ï¼ŒéªŒè¯ä¸æ˜¯åˆ—åé—®é¢˜
        df = df[columns_order].iloc[:,:5]
        
        # ä¿å­˜åˆ°CSVæ–‡ä»¶
        df.to_csv(output_file, index=False, encoding='utf-8')
        print(f"âœ… æˆåŠŸä¿å­˜ {len(data)} æ¡è®°å½•åˆ°: {output_file}")
        
        # éªŒè¯æ–‡ä»¶æ˜¯å¦ä¿å­˜æˆåŠŸ
        if os.path.exists(output_file):
            file_size = os.path.getsize(output_file)
            print(f"ğŸ“Š æ–‡ä»¶å¤§å°: {file_size} bytes")
            
            # è¯»å–å¹¶æ˜¾ç¤ºå‰å‡ è¡Œä½œä¸ºéªŒè¯
            try:
                verify_df = pd.read_csv(output_file, nrows=3)
                print(f"ğŸ“‹ æ–‡ä»¶å†…å®¹é¢„è§ˆ:")
                print(verify_df[['type', 'id', 'title', 'hot']].to_string())
            except Exception as e:
                print(f"âš ï¸ éªŒè¯æ–‡ä»¶å†…å®¹æ—¶å‡ºé”™: {e}")
        else:
            print("âŒ æ–‡ä»¶ä¿å­˜åæœªæ‰¾åˆ°ï¼")
            
    except Exception as e:
        print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def main():
    """
    ä¸»å‡½æ•°ï¼šæ‰§è¡ŒHTMLè§£æå’Œæ•°æ®ä¿å­˜æµç¨‹
    """
    print("ğŸš€ å¼€å§‹æ‰§è¡ŒçŸ¥ä¹çƒ­é—¨é—®é¢˜ç½‘é¡µè§£æè„šæœ¬")
    print("=" * 60)
    
    # è·å–ç”¨æˆ·è¾“å…¥çš„URLï¼ˆå¯ä»¥åœ¨è¿™é‡Œä¿®æ”¹ä¸ºå…·ä½“çš„URLï¼‰
    #target_url = input("ğŸ”— è¯·è¾“å…¥è¦è§£æçš„çŸ¥ä¹é¡µé¢URLï¼ˆæˆ–ç›´æ¥æŒ‰Enterä½¿ç”¨é»˜è®¤æµ‹è¯•URLï¼‰: ").strip()
    target_url = "https://www.zhihu.com/hot"
    # å¦‚æœç”¨æˆ·æ²¡æœ‰è¾“å…¥URLï¼Œå¯ä»¥ä½¿ç”¨é»˜è®¤çš„æµ‹è¯•URLæˆ–æ£€æŸ¥æœ¬åœ°æ–‡ä»¶
    if not target_url:
        print("âš ï¸ æœªè¾“å…¥URLï¼Œå°è¯•ä½¿ç”¨æœ¬åœ°text.htmlæ–‡ä»¶...")
        script_dir = os.path.dirname(os.path.abspath(__file__))
        html_file = os.path.join(script_dir, "text.html")
        
        if os.path.exists(html_file):
            print(f"ğŸ“ ä½¿ç”¨æœ¬åœ°HTMLæ–‡ä»¶: {html_file}")
            results = parse_html_file(html_file)
        else:
            print("âŒ æœ¬åœ°HTMLæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·æä¾›URL")
            return
    else:
        print(f"ğŸŒ ç›®æ ‡URL: {target_url}")
        # ä»URLè·å–å¹¶è§£æHTMLå†…å®¹
        results = parse_html_from_url(target_url)
    
    if results:
        # è®¾ç½®è¾“å‡ºæ–‡ä»¶è·¯å¾„
        script_dir = os.path.dirname(os.path.abspath(__file__))
        output_file = os.path.join(script_dir, "..", "data", "question_list.csv")
        
        # ä¿å­˜ç»“æœåˆ°CSVæ–‡ä»¶

        save_to_csv(results, output_file)
        print("\nğŸ‰ è„šæœ¬æ‰§è¡Œå®Œæˆï¼")
        print(f"ğŸ“‹ è§£æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    else:
        print("\nâŒ æ²¡æœ‰è§£æåˆ°ä»»ä½•æ•°æ®")

if __name__ == "__main__":
    main() 