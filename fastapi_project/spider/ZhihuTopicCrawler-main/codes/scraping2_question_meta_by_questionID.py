"""
çŸ¥ä¹çˆ¬è™« - ç¬¬äºŒæ­¥ï¼šæ ¹æ®é—®é¢˜IDè·å–é—®é¢˜è¯¦ç»†å…ƒæ•°æ®ï¼ˆä¿®å¤ç‰ˆï¼‰
==============================================

ğŸ¯ åŠŸèƒ½è¯´æ˜ï¼š
- è¾“å…¥ï¼šé—®é¢˜IDåˆ—è¡¨
- è¾“å‡ºï¼šé—®é¢˜çš„è¯¦ç»†å…ƒæ•°æ®ï¼ˆé—®é¢˜æ ‡é¢˜ã€å›ç­”æ•°ã€å…³æ³¨æ•°ã€æµè§ˆæ•°ã€æ ‡ç­¾ç­‰ï¼‰
- ç”¨é€”ï¼šè·å–æ¯ä¸ªé—®é¢˜çš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ï¼Œä¸ºåç»­åˆ†ææä¾›åŸºç¡€æ•°æ®

ğŸ“Š æ•°æ®æµç¨‹ï¼š
1. è¯»å– question_list.csvï¼ˆç”±scraping1ç”Ÿæˆï¼‰
2. å¯¹æ¯ä¸ªé—®é¢˜è®¿é—®å…¶è¯¦æƒ…é¡µé¢
3. è§£æHTMLé¡µé¢è·å–å…ƒæ•°æ®
4. ä¿å­˜åˆ° question_meta_info.csv

âš ï¸ æ³¨æ„äº‹é¡¹ï¼š
- éœ€è¦å…ˆè¿è¡Œ scraping1 è·å–é—®é¢˜åˆ—è¡¨
- æ¯æ¬¡æœ€å¥½é™åˆ¶åœ¨250æ¡ä»¥å†…ï¼Œé¿å…è§¦å‘åçˆ¬æœºåˆ¶
- å»ºè®®è®¾ç½®æ—¶é—´è¿‡æ»¤æ¡ä»¶ï¼Œåªçˆ¬å–æœ€è¿‘çš„é—®é¢˜
- å·²ä¿®å¤403é”™è¯¯å’Œç©ºæ–‡ä»¶è¯»å–é—®é¢˜

ä¿®å¤ç‰ˆæœ¬ï¼š2025/06/20
ä¸Šæ¬¡è¿è¡Œï¼š2024/11/16 12:51
"""

import os
import time
import random
import pandas as pd
from bs4 import BeautifulSoup as bs  # HTMLè§£æåº“ï¼Œç”¨äºä»ç½‘é¡µä¸­æå–æ•°æ®
from get_url_text import get_url_text  # è‡ªå®šä¹‰çš„ç½‘ç»œè¯·æ±‚æ¨¡å—

def get_question_list(filename):
    """
    ğŸ“– è¯»å–é—®é¢˜åˆ—è¡¨å¹¶è¿›è¡Œç­›é€‰
    
    å‚æ•°ï¼š
        filename: é—®é¢˜åˆ—è¡¨CSVæ–‡ä»¶è·¯å¾„ï¼ˆé€šå¸¸æ˜¯question_list.csvï¼‰
    
    è¿”å›ï¼š
        q_list: ç­›é€‰åçš„é—®é¢˜åˆ—è¡¨
    
    ç­›é€‰æ¡ä»¶ï¼š
        1. æ’é™¤ä¸“æ æ–‡ç« ï¼ˆtype != "ä¸“æ "ï¼‰
        2. åªä¿ç•™æŒ‡å®šæ—¥æœŸåçš„é—®é¢˜ï¼ˆå¯æ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
    """
    try:
        df = pd.read_csv(filename)
        print(f"âœ… æˆåŠŸè¯»å–é—®é¢˜åˆ—è¡¨æ–‡ä»¶: {filename}")
        print(f"ğŸ“Š åŸå§‹æ•°æ®åŒ…å« {len(df)} æ¡è®°å½•")
        
        # æ˜¾ç¤ºæ–‡ä»¶çš„åˆ—å
        print(f"ğŸ“‹ æ•°æ®åˆ—å: {list(df.columns)}")
        
        # ç­›é€‰æ¡ä»¶1ï¼šæ’é™¤ä¸“æ æ–‡ç« ï¼Œåªä¿ç•™é—®é¢˜å’Œå›ç­”
        if "type" in df.columns:
            original_count = len(df)
            df = df[df["type"] != "ä¸“æ "]
            print(f"ğŸ” æ’é™¤ä¸“æ æ–‡ç« åå‰©ä½™: {len(df)} æ¡è®°å½• (åˆ é™¤äº† {original_count - len(df)} æ¡)")
        
        # ç­›é€‰æ¡ä»¶2ï¼šåªçˆ¬å–æŒ‡å®šæ—¥æœŸä¹‹åçš„é—®é¢˜ï¼ˆå¯æ ¹æ®éœ€è¦ä¿®æ”¹ï¼‰
        if "date" in df.columns:
            original_count = len(df)
            df = df[df["date"] >= "2025-06-15"]  # è°ƒæ•´ä¸ºæ›´è¿‘çš„æ—¥æœŸ
            print(f"ğŸ—“ï¸  ç­›é€‰2025-06-15åçš„é—®é¢˜: {len(df)} æ¡è®°å½• (åˆ é™¤äº† {original_count - len(df)} æ¡)")
        
        # é™åˆ¶æ•°é‡ï¼Œé¿å…è§¦å‘åçˆ¬è™«
        if len(df) > 10:  # é™åˆ¶ä¸º10æ¡ï¼Œæ›´å®‰å…¨
            df = df.head(10)
            print(f"âš ï¸  ä¸ºé¿å…åçˆ¬è™«ï¼Œé™åˆ¶å¤„ç†å‰10æ¡è®°å½•")
        
        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼ï¼Œæ–¹ä¾¿åç»­å¤„ç†
        q_list = df.values.tolist()
        print(f"ğŸ“ æœ€ç»ˆå°†å¤„ç† {len(q_list)} æ¡é—®é¢˜")
        return q_list
        
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ {filename}")
        print("ğŸ’¡ è¯·å…ˆè¿è¡Œ scraping1_questions_by_topicID.py ç”Ÿæˆé—®é¢˜åˆ—è¡¨")
        return []
    except Exception as e:
        print(f"âŒ è¯»å–é—®é¢˜åˆ—è¡¨æ—¶å‡ºé”™: {e}")
        return []

def get_question_data(html_text, q_id):
    """
    ğŸ” ä»HTMLé¡µé¢ä¸­è§£æé—®é¢˜çš„å…ƒæ•°æ®
    
    å‚æ•°ï¼š
        html_text: é—®é¢˜è¯¦æƒ…é¡µçš„HTMLæºç 
        q_id: é—®é¢˜IDï¼ˆç”¨äºé”™è¯¯å¤„ç†ï¼‰
    
    è¿”å›ï¼š
        list: åŒ…å«é—®é¢˜å„é¡¹å…ƒæ•°æ®çš„åˆ—è¡¨
              [é—®é¢˜ID, é—®é¢˜æ ‡é¢˜, å…³æ³¨æ•°, æµè§ˆæ•°, å›ç­”æ•°, è¯é¢˜æ ‡ç­¾, åˆ›å»ºæ—¥æœŸ]
    
    è§£æçš„å…ƒæ•°æ®åŒ…æ‹¬ï¼š
        - qContent: é—®é¢˜æ ‡é¢˜
        - followerCount: å…³æ³¨è¯¥é—®é¢˜çš„äººæ•°
        - viewCount: é—®é¢˜æµè§ˆé‡
        - answerCount: å›ç­”æ•°é‡
        - topicTag: é—®é¢˜æ‰€å±çš„è¯é¢˜æ ‡ç­¾
        - date: é—®é¢˜åˆ›å»ºæ—¥æœŸ
    """

    try:
        bsobj = bs(html_text, "html.parser")
        
        # æ£€æŸ¥æ˜¯å¦æˆåŠŸè·å–åˆ°HTMLå†…å®¹
        if not html_text or len(html_text) < 100:
            print(f"âš ï¸  é—®é¢˜ {q_id} çš„HTMLå†…å®¹å¼‚å¸¸ï¼Œé•¿åº¦: {len(html_text) if html_text else 0}")
            return [q_id, "EmptyHTML", "0", "0", "0", "æœªçŸ¥", "2025-01-01"]

        # å°è¯•æå–é—®é¢˜æ ‡é¢˜
        try:
            qContent_elements = bsobj.find_all("meta", attrs={"itemprop": "name"})
            if qContent_elements:
                qContent = qContent_elements[0].get("content", "æœªçŸ¥æ ‡é¢˜")
            else:
                # å¤‡ç”¨æ–¹æ¡ˆï¼šå°è¯•ä»titleæ ‡ç­¾è·å–
                title_element = bsobj.find("title")
                qContent = title_element.get_text() if title_element else "æœªçŸ¥æ ‡é¢˜"
        except Exception as e:
            print(f"âš ï¸  æå–æ ‡é¢˜å¤±è´¥: {e}")
            qContent = "æ ‡é¢˜æå–å¤±è´¥"

        # å°è¯•æå–å…³æ³¨æ•°å’Œæµè§ˆæ•°
        try:
            number_boards = bsobj.find_all("strong", attrs={"class": "NumberBoard-itemValue"})
            if len(number_boards) >= 2:
                followerCount = number_boards[0].get("title", "0")
                viewCount = number_boards[1].get("title", "0")
            else:
                followerCount = "0"
                viewCount = "0"
        except Exception as e:
            print(f"âš ï¸  æå–å…³æ³¨æ•°/æµè§ˆæ•°å¤±è´¥: {e}")
            followerCount = "0"
            viewCount = "0"

        # å°è¯•æå–å›ç­”æ•°
        try:
            answerCount_elements = bsobj.find_all("meta", attrs={"itemprop": "answerCount"})
            if answerCount_elements:
                answerCount = answerCount_elements[0].get("content", "0")
            else:
                answerCount = "0"
        except Exception as e:
            print(f"âš ï¸  æå–å›ç­”æ•°å¤±è´¥: {e}")
            answerCount = "0"

        # å°è¯•æå–è¯é¢˜æ ‡ç­¾
        try:
            topicTag_elements = bsobj.find_all("meta", attrs={"itemprop": "keywords"})
            if topicTag_elements:
                topicTag = topicTag_elements[0].get("content", "æœªçŸ¥è¯é¢˜")
            else:
                topicTag = "æœªçŸ¥è¯é¢˜"
        except Exception as e:
            print(f"âš ï¸  æå–è¯é¢˜æ ‡ç­¾å¤±è´¥: {e}")
            topicTag = "æœªçŸ¥è¯é¢˜"

        # å°è¯•æå–åˆ›å»ºæ—¥æœŸ
        try:
            date_elements = bsobj.find_all("meta", attrs={"itemprop": "dateCreated"})
            if date_elements:
                date = date_elements[0].get("content", "2025-01-01")
                date = date[:10]  # åªå–æ—¥æœŸéƒ¨åˆ†
            else:
                date = "2025-01-01"
        except Exception as e:
            print(f"âš ï¸  æå–åˆ›å»ºæ—¥æœŸå¤±è´¥: {e}")
            date = "2025-01-01"

        print(f"âœ… æˆåŠŸè§£æé—®é¢˜ {q_id}: {qContent[:30]}...")
        return [q_id, qContent, followerCount, viewCount, answerCount, topicTag, date]

    except Exception as e:
        print(f"âŒ è§£æé—®é¢˜ {q_id} æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        return [q_id, "ParseError", "0", "0", "0", "è§£æé”™è¯¯", "2025-01-01"]

def save_data(q_info_list, filename):
    """
    ğŸ’¾ ä¿å­˜é—®é¢˜å…ƒæ•°æ®åˆ°CSVæ–‡ä»¶ï¼ˆä¿®å¤ç‰ˆï¼‰
    
    å‚æ•°ï¼š
        q_info_list: é—®é¢˜ä¿¡æ¯åˆ—è¡¨
        filename: è¾“å‡ºæ–‡ä»¶åï¼ˆé€šå¸¸æ˜¯question_meta_info.csvï¼‰
    
    æ•°æ®å¤„ç†é€»è¾‘ï¼š
        1. åˆ›å»ºDataFrameå¹¶è®¾ç½®åˆ—å
        2. å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ä¸”ä¸ä¸ºç©ºï¼Œåˆ™åˆå¹¶æ–°æ—§æ•°æ®
        3. æ¸…ç†æ— æ•ˆæ•°æ®ï¼ˆUnknownErrorï¼‰
        4. æŒ‰é—®é¢˜IDå»é‡ï¼Œä¿ç•™æœ€æ–°æ•°æ®
        5. æ ¼å¼åŒ–æ—¥æœŸå¹¶æ’åº
        6. ä¿å­˜åˆ°CSVæ–‡ä»¶
    """
    if not q_info_list:
        print("âš ï¸  æ²¡æœ‰æ•°æ®éœ€è¦ä¿å­˜")
        return
        
    # åˆ›å»ºDataFrameï¼Œè®¾ç½®åˆ—å
    df = pd.DataFrame(
        q_info_list,
        columns=[
            "q_id",           # é—®é¢˜ID
            "q_content",      # é—®é¢˜æ ‡é¢˜
            "followerCount",  # å…³æ³¨æ•°
            "viewCount",      # æµè§ˆæ•°
            "answerCount",    # å›ç­”æ•°
            "topicTag",       # è¯é¢˜æ ‡ç­¾
            "created_date"    # åˆ›å»ºæ—¥æœŸ
        ],
    )
    
    print(f"ğŸ“Š å‡†å¤‡ä¿å­˜ {len(df)} æ¡è®°å½•åˆ° {filename}")
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    
    # å¦‚æœæ–‡ä»¶å·²ç»å­˜åœ¨ä¸”ä¸ä¸ºç©ºï¼Œåˆå¹¶æ–°æ—§æ•°æ®
    if os.path.exists(filename) and os.path.getsize(filename) > 0:
        try:
            df_old = pd.read_csv(filename)
            print(f"ğŸ“– è¯»å–åˆ°æ—§æ•°æ® {len(df_old)} æ¡è®°å½•")
            df = pd.concat([df_old, df], ignore_index=True)
            
            # æ•°æ®æ¸…ç†ï¼šåˆ é™¤è§£æå¤±è´¥çš„é—®é¢˜ï¼ˆæ ‡è®°ä¸ºUnknownErrorã€EmptyHTMLç­‰çš„è®°å½•ï¼‰
            error_conditions = df["q_content"].isin(["UnknownError", "EmptyHTML", "ParseError", "æ ‡é¢˜æå–å¤±è´¥"])
            error_count = error_conditions.sum()
            if error_count > 0:
                print(f"ğŸ§¹ æ¸…ç† {error_count} æ¡é”™è¯¯è®°å½•")
                df = df[~error_conditions]
            
            # æŒ‰é—®é¢˜IDå»é‡ï¼Œä¿ç•™æœ€æ–°çš„æ•°æ®
            original_count = len(df)
            df = df.drop_duplicates(subset=["q_id"], keep="last")
            duplicate_count = original_count - len(df)
            if duplicate_count > 0:
                print(f"ğŸ”„ å»é‡ {duplicate_count} æ¡é‡å¤è®°å½•")
            
        except Exception as e:
            print(f"âš ï¸  è¯»å–æ—§æ–‡ä»¶å¤±è´¥ï¼Œå°†åˆ›å»ºæ–°æ–‡ä»¶: {e}")
    
    try:
        # æ—¥æœŸæ ¼å¼åŒ–ï¼šç¡®ä¿æ—¥æœŸæ ¼å¼ä¸€è‡´
        df["created_date"] = pd.to_datetime(df["created_date"], errors='coerce')
        df["created_date"] = df["created_date"].dt.strftime('%Y-%m-%d')
        
        # æŒ‰åˆ›å»ºæ—¥æœŸæ’åº
        df = df.sort_values(by=["created_date"], ascending=False)  # æœ€æ–°çš„åœ¨å‰
        
        # ä¿å­˜åˆ°CSVæ–‡ä»¶ï¼Œä½¿ç”¨UTF-8ç¼–ç ä»¥æ”¯æŒä¸­æ–‡
        df.to_csv(filename, index=False, header=True, encoding="utf-8")
        print(f"âœ… æˆåŠŸä¿å­˜ {len(df)} æ¡è®°å½•åˆ° {filename}")
        
        # æ˜¾ç¤ºä¿å­˜çš„æ•°æ®é¢„è§ˆ
        if len(df) > 0:
            print("ğŸ“‹ æ•°æ®é¢„è§ˆï¼ˆæœ€æ–°3æ¡ï¼‰:")
            preview_df = df.head(3)[['q_id', 'q_content', 'answerCount', 'followerCount']]
            for idx, row in preview_df.iterrows():
                print(f"  - {row['q_id']}: {row['q_content'][:40]}... (å›ç­”:{row['answerCount']}, å…³æ³¨:{row['followerCount']})")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def main():
    """
    ä¸»å‡½æ•°ï¼šæ‰§è¡Œé—®é¢˜å…ƒæ•°æ®çˆ¬å–æµç¨‹
    """
    print("ğŸš€ å¼€å§‹æ‰§è¡ŒçŸ¥ä¹é—®é¢˜å…ƒæ•°æ®çˆ¬å–è„šæœ¬")
    print("=" * 60)
    
    # è¯»å–é—®é¢˜åˆ—è¡¨
    q_list = get_question_list("data/question_list.csv")
    if not q_list:
        print("âŒ æ— æ³•è·å–é—®é¢˜åˆ—è¡¨ï¼Œç¨‹åºé€€å‡º")
        return
    
    print(f"ğŸ“Š å…± {len(q_list)} ä¸ªé—®é¢˜å¾…å¤„ç†")
    q_info_list = []
    success_count = 0
    error_count = 0

    # å¤„ç†æ¯ä¸ªé—®é¢˜
    for i, item in enumerate(q_list):
        try:
            # è·å–é—®é¢˜IDï¼ˆæ ¹æ®å®é™…CSVç»“æ„è°ƒæ•´ç´¢å¼•ï¼‰
            if len(item) > 1:
                q_id = item[1]  # å‡è®¾IDåœ¨ç¬¬äºŒåˆ—
            else:
                q_id = item[0]  # å¦‚æœåªæœ‰ä¸€åˆ—
            
            print(f"\nğŸ” å¤„ç†é—®é¢˜ {i+1}/{len(q_list)}: ID={q_id}")
            
            # æ„å»ºé—®é¢˜URL
            url = f"https://www.zhihu.com/question/{str(q_id)}"
            
            # è·å–ç½‘é¡µå†…å®¹
            text = get_url_text(url)
            
            if not text:
                print(f"âŒ æ— æ³•è·å–é—®é¢˜ {q_id} çš„ç½‘é¡µå†…å®¹")
                error_count += 1
                # æ·»åŠ é”™è¯¯è®°å½•
                q_info_list.append([q_id, "NetworkError", "0", "0", "0", "ç½‘ç»œé”™è¯¯", "2025-01-01"])
                continue
            
            # è§£æé—®é¢˜æ•°æ®
            q_info = get_question_data(text, q_id)
            q_info_list.append(q_info)
            
            if q_info[1] not in ["NetworkError", "EmptyHTML", "ParseError", "æ ‡é¢˜æå–å¤±è´¥"]:
                success_count += 1
            else:
                error_count += 1

            # æ¯å¤„ç†30ä¸ªé—®é¢˜ä¿å­˜ä¸€æ¬¡ï¼Œé¿å…æ•°æ®ä¸¢å¤±
            if (i + 1) % 30 == 0 or (i + 1) == len(q_list):
                print(f"\nğŸ’¾ ä¿å­˜è¿›åº¦ï¼šå·²å¤„ç† {i+1} ä¸ªé—®é¢˜")
                save_data(q_info_list, "data/question_meta_info.csv")
                q_info_list = []  # æ¸…ç©ºåˆ—è¡¨ï¼Œå‡†å¤‡ä¸‹ä¸€æ‰¹
                
                # æ·»åŠ éšæœºå»¶è¿Ÿï¼Œé¿å…è§¦å‘åçˆ¬è™«
                delay = random.uniform(2, 5)
                print(f"â° ä¼‘æ¯ {delay:.1f} ç§’...")
                time.sleep(delay)

        except Exception as e:
            print(f"âŒ å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {e}")
            error_count += 1
            continue

    # æœ€ç»ˆä¿å­˜
    if q_info_list:
        save_data(q_info_list, "data/question_meta_info.csv")

    print(f"\nğŸ‰ è„šæœ¬æ‰§è¡Œå®Œæˆï¼")
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  - æ€»å¤„ç†æ•°: {len(q_list)}")
    print(f"  - æˆåŠŸæ•°: {success_count}")
    print(f"  - å¤±è´¥æ•°: {error_count}")
    print(f"  - æˆåŠŸç‡: {success_count/(len(q_list)) * 100:.1f}%")

# ä»£ç ä¸€æ¬¡åªèƒ½è·‘250æ¡ï¼Œä¹‹åä¼šå˜ä¹±ç ï¼Œéœ€è¦æ‰‹åŠ¨å»æµè§ˆå™¨æ›´æ–°cookie
# 2024/11/16æ›´æ–°ï¼šä¼¼ä¹ä¸ä¼šå†å˜ä¹±ç äº†ï¼Œå»ºè®®ä¿æŒå…³æ³¨
# 2025/06/20æ›´æ–°ï¼šä¿®å¤äº†403é”™è¯¯å’Œç©ºæ–‡ä»¶è¯»å–é—®é¢˜
if __name__ == "__main__":
    main()
