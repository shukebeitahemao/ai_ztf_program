"""
çŸ¥ä¹çˆ¬è™« - ç¬¬ä¸‰æ­¥ï¼šæ ¹æ®é—®é¢˜IDè·å–æ‰€æœ‰å›ç­”çš„è¯¦ç»†ä¿¡æ¯
===============================================

ğŸ¯ åŠŸèƒ½è¯´æ˜ï¼š
- è¾“å…¥ï¼šé—®é¢˜IDåˆ—è¡¨
- è¾“å‡ºï¼šæ¯ä¸ªé—®é¢˜ä¸‹æ‰€æœ‰å›ç­”çš„è¯¦ç»†ä¿¡æ¯
- ç”¨é€”ï¼šè·å–å›ç­”å†…å®¹ã€ä½œè€…ä¿¡æ¯ã€ç‚¹èµæ•°ã€è¯„è®ºæ•°ç­‰è¯¦ç»†æ•°æ®

ğŸ“Š æ•°æ®æµç¨‹ï¼š
1. è¯»å– question_meta_info.csvï¼ˆç”±scraping2ç”Ÿæˆï¼‰
2. å¯¹æ¯ä¸ªé—®é¢˜è°ƒç”¨çŸ¥ä¹APIè·å–å›ç­”åˆ—è¡¨
3. è§£æJSONæ•°æ®æå–å›ç­”è¯¦æƒ…å’Œä½œè€…ä¿¡æ¯
4. ä¿å­˜åˆ° data/answers_of_question/question_{é—®é¢˜ID}.csv

ğŸš€ æ ¸å¿ƒAPIï¼š
- https://www.zhihu.com/api/v4/questions/{é—®é¢˜ID}/feeds
- æ”¯æŒåˆ†é¡µè·å–æ‰€æœ‰å›ç­”
- åŒ…å«ä½œè€…è¯¦ç»†ä¿¡æ¯å’Œå›ç­”ç»Ÿè®¡æ•°æ®

âš ï¸ æ³¨æ„äº‹é¡¹ï¼š
- éœ€è¦å…ˆè¿è¡Œ scraping2 è·å–é—®é¢˜å…ƒæ•°æ®
- çˆ¬å–æ—¶é—´è¾ƒé•¿ï¼Œæ”¯æŒæ–­ç‚¹ç»­çˆ¬
- ä¼šè§¦å‘éªŒè¯ç æœºåˆ¶ï¼Œéœ€è¦å®šæœŸæ›´æ–°cookie

ä¸Šæ¬¡è¿è¡Œï¼š2024/11/16 12:55
"""

import re  # æ­£åˆ™è¡¨è¾¾å¼ï¼Œç”¨äºæ¸…ç†HTMLæ ‡ç­¾
import os
import json  # å¤„ç†APIè¿”å›çš„JSONæ•°æ®
import time
import pandas as pd
from datetime import datetime  # æ—¶é—´æˆ³è½¬æ¢
from get_url_text import get_url_text  # è‡ªå®šä¹‰çš„ç½‘ç»œè¯·æ±‚æ¨¡å—

def get_q_list(filename):
    df = pd.read_csv(filename, encoding="utf-8")
    df = df[df["answerCount"] > 5]  # é»˜è®¤çˆ¬å–å›ç­”æ•°å¤§äº5çš„é—®é¢˜
    df = df[df["created_date"] >= "2024-10-15"]  # å¯é€‰è¦æ›´æ–°çš„é—®é¢˜çš„æ—¶é—´èŒƒå›´
    q_list = df.values.tolist()
    print(f"å…±æœ‰ {len(q_list)} ä¸ªå›ç­”æ•°å¤§äº5ä¸”ä¸é‡å¤çš„é—®é¢˜")

    return q_list[::-1]  # ä»åå¾€å‰çˆ¬

def parse_data(html, q_id):

    json_data = json.loads(html)["data"]
    next_url = json.loads(html)["paging"]["next"]
    is_end = json.loads(html)["paging"]["is_end"]

    one_q_all_answer = []

    try:
        for item in json_data:
            one_answer_list = []

            question_id = q_id  # Question id
            answer_content = item["target"]["content"]
            answer_content = re.sub("<[^<]+?>", "", answer_content)
            answer_date = datetime.fromtimestamp(item["target"]["created_time"]).strftime(
                "%Y-%m-%d"
            )  # Answer date
            answer_upvote = item["target"]["voteup_count"]  # upvote count
            answer_comment = item["target"]["comment_count"]  # comment count
            answer_id = item["target"]["id"]  # answer ID
            author_name = item["target"]["author"]["name"]  # author name
            author_gender = item["target"]["author"][
                "gender"
            ]  # author gender, 1 male 2 female
            author_url_token = item["target"]["author"]["url_token"]  # author ID
            author_follower_count = item["target"]["author"][
                "follower_count"
            ]  # author follower count
            author_headline = item["target"]["author"]["headline"]  # author bio

            one_answer_list = [
                question_id,
                answer_content,
                answer_date,
                answer_upvote,
                answer_comment,
                answer_id,
                author_name,
                author_gender,
                author_url_token,
                author_follower_count,
                author_headline,
            ]
            one_q_all_answer.append(one_answer_list)

        return one_q_all_answer, next_url, is_end

    except Exception as e:
        print(one_q_all_answer)
        print(e)

def save_data(answer_info, q_id):

    filename = f"data/answers_of_question/question_{str(q_id)}.csv"

    df = pd.DataFrame(
        answer_info,
        columns=[
            "q_id",
            "a_content",
            "a_date",
            "a_upvote",
            "a_comment",
            "a_id",
            "au_name",
            "au_gender",
            "au_urltoken",
            "au_followerCount",
            "au_headline",
        ],
    )
    if os.path.exists(filename):
        df_original = pd.read_csv(filename)
        df = pd.concat([df_original, df], ignore_index=True)
        df = df.drop_duplicates(subset=["a_id"]).sort_values(by="a_date")

    df.to_csv(filename, index=False, header=True)

if __name__ == "__main__":
    # TODO: æŒ‡å®šé—®é¢˜åˆ—è¡¨
    q_list = get_q_list("data/question_meta_info.csv")
    # ä¹Ÿå¯æ‰‹åŠ¨è¾“å…¥é—®é¢˜ ID ä»¥è·å–å›ç­”æ•°æ®
    # q_list = ["291278869", "291278870"]

    # çˆ¬ä¸€æ®µæ—¶é—´ä¼šè§¦å‘çŸ¥ä¹çš„éªŒè¯ç æœºåˆ¶å¯¼è‡´HTTPErroræŠ¥é”™ï¼Œéœ€è¦æ‰‹åŠ¨é‡æ–°è®¾ç½®å¼€å§‹ä½ç½®
    begin_index = 0  # å°†å‘ç”ŸæŠ¥é”™çš„é—®é¢˜åºå·æ›´æ–°åˆ°è¿™é‡Œå³å¯
    for i, item in enumerate(q_list[begin_index:]):  
        q_id = item[0]

        print(f"\nquestion {i+begin_index} {item[1]} Begin, qid: {q_id}")

        url = f"https://www.zhihu.com/api/v4/questions/{str(q_id)}/feeds?include=content%2Cauthor.follower_count"

        if_question_exist = os.path.exists(f"data/answers_of_question/question_{str(q_id)}.csv")
        get_data_by_time = False # çˆ¬è™«ä¸­æ˜¯å¦æŒ‰æ—¶é—´æ’åº

        # âš ï¸âš ï¸âš ï¸è‹¥æŒ‰æ—¶é—´æ’åºæ›´æ–°æ•°æ®ä¸­å‘ç”ŸæŠ¥é”™ï¼Œåˆ™éœ€è¦åˆ é™¤è¯¥é—®é¢˜çš„å¯¹åº”çš„CSVæ–‡ä»¶ï¼Œé‡æ–°çˆ¬å–âš ï¸âš ï¸

        if if_question_exist:
            data_existing = pd.read_csv(f"data/answers_of_question/question_{str(q_id)}.csv")
            a_id_existing = data_existing["a_id"].values.tolist()

            try:
                # å·²æœ‰æ•°æ®çš„æ—§é—®é¢˜å°è¯•æŒ‰æ—¶é—´æ’åºï¼ŒèŠ‚çœæ—¶é—´
                text = get_url_text(url + "&order=updated")
                data, url, is_end = parse_data(text, q_id)
                url = url + "&order=updated"
                get_data_by_time = True
            except:
                # è‹¥ä¸èƒ½æŒ‰æ—¶é—´æ’åºï¼Œåˆ™æŒ‰é»˜è®¤é¡ºåº
                pass

        # å¯¹äºå›ç­”æ•°å¾ˆå¤šçš„é—®é¢˜ï¼ŒæŠ¥é”™æ—¶å¯åœ¨æ­¤å¤„æ·»åŠ ä¸­é€”urlï¼Œæ–¹ä¾¿æ–­ç‚¹ç»­çˆ¬
        # url = "" # æ”¾å…¥æŠ¥é”™å‰æœ€åè¾“å‡ºçš„url
        #TODO

        page = 0
        is_end = False
        while not is_end:
            text = get_url_text(url)
            data, url, is_end = parse_data(text, q_id)

            save_data(data, q_id)

            if get_data_by_time:
                # æŒ‰æ—¶é—´æ’åºæ—¶ï¼Œè‹¥æ‰€æœ‰æ•°æ®éƒ½å·²çˆ¬å–ï¼Œè·³å‡ºå¾ªç¯ï¼Œæ›´æ–°å®Œæˆ
                a_id = [item[5] for item in data]
                if all(item in a_id_existing for item in a_id):
                    break

            page += 1
            if page % 10 == 0:
                time.sleep(0.5)
                try:
                    print(url)
                    print(f"æ–‡æœ¬ç¤ºä¾‹ï¼š{data[-1][1][:15]}")
                except:
                    pass

        print(f"\nquestion {i+begin_index} {item[1]} Finish")

    print("Finish!!")
