"""
çŸ¥ä¹çˆ¬è™« - ç¬¬ä¸€æ­¥ï¼šæ ¹æ®è¯é¢˜IDè·å–é—®é¢˜åˆ—è¡¨
=====================================

ğŸ¯ åŠŸèƒ½è¯´æ˜ï¼š
- è¾“å…¥ï¼šçŸ¥ä¹è¯é¢˜IDåˆ—è¡¨
- è¾“å‡ºï¼šè¯¥è¯é¢˜ä¸‹çš„æ‰€æœ‰é—®é¢˜åˆ—è¡¨ï¼ˆåŒ…å«é—®é¢˜IDã€æ ‡é¢˜ã€URLã€æ—¥æœŸç­‰ï¼‰
- ç”¨é€”ï¼šè¿™æ˜¯æ•´ä¸ªçˆ¬è™«æµç¨‹çš„ç¬¬ä¸€æ­¥ï¼Œä¸ºåç»­æ­¥éª¤æä¾›é—®é¢˜æ•°æ®æº

ğŸ“Š æ•°æ®æ¥æºï¼š
- è®¨è®ºåŒºï¼šessence + timeline_activity
- ç²¾ååŒºï¼štop_activity  
- ç­‰å¾…å›ç­”ï¼štop_question + new_question

ğŸš€ æ ¸å¿ƒAPIï¼š
- https://www.zhihu.com/api/v5.1/topics/{è¯é¢˜ID}/feeds/{ç±»å‹}
- æ”¯æŒåˆ†é¡µè·å–ï¼Œè‡ªåŠ¨å¤„ç†æ‰€æœ‰é¡µé¢

âš ï¸ æ³¨æ„äº‹é¡¹ï¼š
- éœ€è¦æœ‰æ•ˆçš„cookieæ‰èƒ½è®¿é—®API
- å¯èƒ½ä¼šè§¦å‘403é”™è¯¯ï¼Œéœ€è¦æ›´æ–°è®¤è¯ä¿¡æ¯
- æ”¯æŒå¤šä¸ªè¯é¢˜IDåŒæ—¶çˆ¬å–

ä¸Šæ¬¡è¿è¡Œï¼š2024/11/16 12:32
"""


import os
import json
import pandas as pd
from datetime import datetime
from get_url_text import get_url_text



def parseJson(text, q_list):
    """
    ğŸ” è§£æçŸ¥ä¹APIè¿”å›çš„JSONæ•°æ®ï¼Œæå–é—®é¢˜ã€å›ç­”ã€æ–‡ç« ä¿¡æ¯
    
    å‚æ•°:
        text: ä»çŸ¥ä¹APIè·å–çš„JSONå­—ç¬¦ä¸²
        q_list: é—®é¢˜åˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨è§£æç»“æœ
    
    è¿”å›:
        nextUrl: ä¸‹ä¸€é¡µçš„URLé“¾æ¥ï¼Œç”¨äºåˆ†é¡µçˆ¬å–
    """
    # å°†JSONå­—ç¬¦ä¸²è½¬æ¢ä¸ºPythonå­—å…¸å¯¹è±¡
    json_data = json.loads(text)
    
    # æå–æ•°æ®åˆ—è¡¨ï¼ŒåŒ…å«å½“å‰é¡µé¢çš„æ‰€æœ‰å†…å®¹é¡¹
    lst = json_data["data"]
    
    # æå–åˆ†é¡µä¿¡æ¯ä¸­çš„ä¸‹ä¸€é¡µURLï¼Œç”¨äºç»§ç»­çˆ¬å–åç»­é¡µé¢
    nextUrl = json_data["paging"]["next"]

    # å¦‚æœå½“å‰é¡µé¢æ²¡æœ‰æ•°æ®ï¼Œç›´æ¥è¿”å›ï¼ˆé˜²æ­¢ç©ºé¡µé¢ç»§ç»­å¤„ç†ï¼‰
    if not lst:
        return

    # éå†å½“å‰é¡µé¢çš„æ¯ä¸€ä¸ªå†…å®¹é¡¹ï¼ˆé—®é¢˜ã€å›ç­”æˆ–æ–‡ç« ï¼‰
    for item in lst:
        # è·å–å†…å®¹ç±»å‹ï¼šanswer(å›ç­”)ã€question(é—®é¢˜)ã€article(æ–‡ç« )
        type = item["target"]["type"]

        if type == "answer":
            # ğŸ¯ å¤„ç†"å›ç­”"ç±»å‹çš„æ•°æ®
            # è¿™ç§æƒ…å†µæ˜¯é€šè¿‡å›ç­”å‘ç°çš„é—®é¢˜ï¼Œæ ‡è®°ä¸ºç‰¹æ®Šç±»å‹
            cn_type = "é—®é¢˜_æ¥è‡ªå›ç­”"
            
            # ä»å›ç­”æ•°æ®ä¸­æå–å¯¹åº”çš„é—®é¢˜ä¿¡æ¯
            question = item["target"]["question"]
            
            # æå–é—®é¢˜çš„å”¯ä¸€æ ‡è¯†ç¬¦
            id = question["id"]
            
            # æå–é—®é¢˜çš„æ ‡é¢˜æ–‡æœ¬
            title = question["title"]
            
            # æ„é€ é—®é¢˜çš„å®Œæ•´URLé“¾æ¥
            url = "https://www.zhihu.com/question/" + str(id)
            
            # å°†Unixæ—¶é—´æˆ³è½¬æ¢ä¸ºå¯è¯»çš„æ—¥æœŸæ ¼å¼(YYYY-MM-DD)
            question_date = datetime.fromtimestamp(question["created"]).strftime(
                "%Y-%m-%d"
            )
            
            # æ„é€ æ•°æ®åˆ—è¡¨ï¼š[ç±»å‹, ID, æ ‡é¢˜, URL, æ—¥æœŸ]
            sml_list = [cn_type, id, title, url, question_date]
            
            # å°†æ•°æ®æ·»åŠ åˆ°å…¨å±€é—®é¢˜åˆ—è¡¨ä¸­
            q_list.append(sml_list)

        elif type == "question":
            # ğŸ¯ å¤„ç†"é—®é¢˜"ç±»å‹çš„æ•°æ®
            # è¿™æ˜¯ç›´æ¥ä»è¯é¢˜é¡µé¢è·å–çš„é—®é¢˜
            cn_type = "é—®é¢˜"
            
            # é—®é¢˜æ•°æ®å°±åœ¨targetå±‚çº§ä¸­ï¼Œä¸éœ€è¦å†ä¸‹ä¸€å±‚
            question = item["target"]
            
            # æå–é—®é¢˜çš„å”¯ä¸€æ ‡è¯†ç¬¦
            id = question["id"]
            
            # æå–é—®é¢˜çš„æ ‡é¢˜æ–‡æœ¬
            title = question["title"]
            
            # æ„é€ é—®é¢˜çš„å®Œæ•´URLé“¾æ¥
            url = "https://www.zhihu.com/question/" + str(id)
            
            # å°†Unixæ—¶é—´æˆ³è½¬æ¢ä¸ºå¯è¯»çš„æ—¥æœŸæ ¼å¼(YYYY-MM-DD)
            question_date = datetime.fromtimestamp(question["created"]).strftime(
                "%Y-%m-%d"
            )
            
            # æ„é€ æ•°æ®åˆ—è¡¨ï¼š[ç±»å‹, ID, æ ‡é¢˜, URL, æ—¥æœŸ]
            sml_list = [cn_type, id, title, url, question_date]
            
            # å°†æ•°æ®æ·»åŠ åˆ°å…¨å±€é—®é¢˜åˆ—è¡¨ä¸­
            q_list.append(sml_list)

        elif type == "article":
            # ğŸ¯ å¤„ç†"æ–‡ç« "ç±»å‹çš„æ•°æ®
            # è¿™æ˜¯çŸ¥ä¹ä¸“æ æ–‡ç« ï¼Œä¸æ˜¯é—®é¢˜
            cn_type = "ä¸“æ "
            
            # ä¸“æ æ–‡ç« æ•°æ®åœ¨targetå±‚çº§ä¸­
            zhuanlan = item["target"]
            
            # æå–æ–‡ç« çš„å”¯ä¸€æ ‡è¯†ç¬¦
            id = zhuanlan["id"]
            
            # æå–æ–‡ç« çš„æ ‡é¢˜
            title = zhuanlan["title"]
            
            # æ–‡ç« URLå·²ç»æ˜¯å®Œæ•´çš„ï¼Œç›´æ¥ä½¿ç”¨
            url = zhuanlan["url"]
            
            # å°†Unixæ—¶é—´æˆ³è½¬æ¢ä¸ºå¯è¯»çš„æ—¥æœŸæ ¼å¼(YYYY-MM-DD)
            article_date = datetime.fromtimestamp(zhuanlan["created"]).strftime(
                "%Y-%m-%d"
            )
            
            # æ„é€ æ•°æ®åˆ—è¡¨ï¼š[ç±»å‹, ID, æ ‡é¢˜, URL, æ—¥æœŸ]
            sml_list = [cn_type, id, title, url, article_date]
            
            # å°†æ•°æ®æ·»åŠ åˆ°å…¨å±€é—®é¢˜åˆ—è¡¨ä¸­
            q_list.append(sml_list)

    # è¿”å›ä¸‹ä¸€é¡µçš„URLï¼Œä¾›å¤–éƒ¨å¾ªç¯ç»§ç»­çˆ¬å–
    # å¦‚æœæ²¡æœ‰ä¸‹ä¸€é¡µï¼ŒAPIä¼šè¿”å›ç©ºå€¼ï¼Œå¾ªç¯ä¼šè‡ªåŠ¨ç»“æŸ
    return nextUrl
# def save_data(q_list, filename):
#     # è·å–ç»å¯¹è·¯å¾„
#     abs_filename = os.path.abspath(filename)
#     print(f"å°è¯•ä¿å­˜åˆ°ç»å¯¹è·¯å¾„: {abs_filename}")
    
#     # ç¡®ä¿ç›®å½•å­˜åœ¨
#     dir_path = os.path.dirname(abs_filename)
#     if not os.path.exists(dir_path):
#         os.makedirs(dir_path, exist_ok=True)
#         print(f"åˆ›å»ºç›®å½•: {dir_path}")
#     else:
#         print(f"ç›®å½•å·²å­˜åœ¨: {dir_path}")

#     df = pd.DataFrame(q_list, columns=["type", "id", "title", "url", "date"])
#     # æ ¹æ®idå»é‡ï¼Œå¹¶æŒ‰ç…§æ—¶é—´æ’åº
#     df = df.drop_duplicates(subset=["id"]).sort_values(by="date")

#     # è‹¥æ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆ™è¯»å–åŸæ–‡ä»¶ï¼Œåˆå¹¶åå»é‡ï¼Œå®ç°æ–‡ä»¶æ›´æ–°
#     if os.path.exists(abs_filename):
#         try:
#             df_original = pd.read_csv(abs_filename)
#             df = pd.concat([df_original, df], ignore_index=True)
#             df = df.drop_duplicates(subset=["id"]).sort_values(by="date")
#             print(f"åˆå¹¶åŸæœ‰æ•°æ®ï¼ŒåŸæœ‰{len(df_original)}æ¡")
#         except Exception as e:
#             print(f"è¯»å–åŸæ–‡ä»¶å¤±è´¥: {e}")

#     try:
#         df.to_csv(abs_filename, index=False, header=True, encoding="utf-8")
#         print(f"âœ… æˆåŠŸä¿å­˜{len(df)}æ¡æ•°æ®åˆ°: {abs_filename}")
        
#         # éªŒè¯æ–‡ä»¶æ˜¯å¦çœŸçš„å­˜åœ¨
#         if os.path.exists(abs_filename):
#             file_size = os.path.getsize(abs_filename)
#             print(f"âœ… æ–‡ä»¶ç¡®è®¤å­˜åœ¨ï¼Œå¤§å°: {file_size} bytes")
#         else:
#             print("âŒ è­¦å‘Šï¼šæ–‡ä»¶ä¿å­˜åæœªæ‰¾åˆ°ï¼")
            
#     except Exception as e:
#         print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
#         import traceback
#         traceback.print_exc()



def save_data(q_list, filename):
    """
    ğŸ’¾ ä¿å­˜é—®é¢˜æ•°æ®åˆ°CSVæ–‡ä»¶
    
    å‚æ•°:
        q_list: é—®é¢˜æ•°æ®åˆ—è¡¨
        filename: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    """
    print(f"ğŸ” å‡†å¤‡ä¿å­˜æ•°æ®ï¼Œå½“å‰q_listé•¿åº¦: {len(q_list)}")
    
    # å¦‚æœq_listä¸ºç©ºï¼Œç›´æ¥è¿”å›
    if not q_list:
        print("âŒ è­¦å‘Šï¼šq_listä¸ºç©ºï¼Œæ— æ³•ä¿å­˜æ•°æ®ï¼")
        return
    
    # æ˜¾ç¤ºå‰å‡ æ¡æ•°æ®ç”¨äºè°ƒè¯•
    print(f"ğŸ“Š æ•°æ®æ ·ä¾‹ï¼ˆå‰3æ¡ï¼‰:")
    for i, item in enumerate(q_list[:3]):
        print(f"   {i+1}: {item}")

    df = pd.DataFrame(q_list, columns=["type", "id", "title", "url", "date"])
    print(f"ğŸ“‹ DataFrameåˆ›å»ºæˆåŠŸï¼Œå½¢çŠ¶: {df.shape}")
    
    # æ ¹æ®idå»é‡ï¼Œå¹¶æŒ‰ç…§æ—¶é—´æ’åº
    df = df.drop_duplicates(subset=["id"]).sort_values(by="date")
    print(f"ğŸ”„ å»é‡åæ•°æ®é‡: {len(df)}")

    # è‹¥æ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆ™è¯»å–åŸæ–‡ä»¶ï¼Œåˆå¹¶åå»é‡ï¼Œå®ç°æ–‡ä»¶æ›´æ–°
    if os.path.exists(filename):
        try:
            df_original = pd.read_csv(filename)
            print(f"ğŸ“ è¯»å–åŸæ–‡ä»¶ï¼ŒåŸæœ‰{len(df_original)}æ¡æ•°æ®")
            df = pd.concat([df_original, df], ignore_index=True)
            df = df.drop_duplicates(subset=["id"]).sort_values(by="date")
            print(f"ğŸ”— åˆå¹¶åæ•°æ®é‡: {len(df)}")
        except Exception as e:
            print(f"âŒ è¯»å–åŸæ–‡ä»¶å¤±è´¥: {e}")

    try:
        df.to_csv(filename, index=False, header=True, encoding="utf-8")
        print(f"âœ… æˆåŠŸä¿å­˜{len(df)}æ¡æ•°æ®åˆ°{filename}")
        
        # éªŒè¯æ–‡ä»¶
        if os.path.exists(filename):
            file_size = os.path.getsize(filename)
            print(f"âœ… æ–‡ä»¶ç¡®è®¤å­˜åœ¨ï¼Œå¤§å°: {file_size} bytes")
        else:
            print("âŒ è­¦å‘Šï¼šæ–‡ä»¶ä¿å­˜åæœªæ‰¾åˆ°ï¼")
    except Exception as e:
        print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def crawl_1(topicID, q_list):
    """
    ğŸ¯ çˆ¬å–è¯é¢˜çš„è®¨è®ºåŒºå†…å®¹
    
    å‚æ•°:
        topicID: è¯é¢˜ID
        q_list: é—®é¢˜åˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨çˆ¬å–ç»“æœ
    """
    # ä»Discussionä¸­è·å–ï¼Œé™åˆ¶50æ¡
    url = (
        "https://www.zhihu.com/api/v5.1/topics/"
        + topicID
        + "/feeds/essence?offset=0&limit=50"
    )
    while url:
        try:
            text = get_url_text(url)
            if text:  # ç¡®ä¿è·å–åˆ°å†…å®¹æ‰è§£æ
                url = parseJson(text, q_list)
            else:
                break
        except:
            print(f"ç›®å‰å·²æœ‰{len(q_list)}æ¡æ•°æ®")
            break

    url = (
        "https://www.zhihu.com/api/v5.1/topics/"
        + topicID
        + "/feeds/timeline_activity?offset=0&limit=50"
    )
    while url:
        try:
            text = get_url_text(url)
            if text:  # ç¡®ä¿è·å–åˆ°å†…å®¹æ‰è§£æ
                url = parseJson(text, q_list)
            else:
                break
        except:
            print(f"ç›®å‰å·²æœ‰{len(q_list)}æ¡æ•°æ®")
            break

    print("crawl_è®¨è®º: å®Œæˆ")

def crawl_2(topicID, q_list):
    """
    ğŸ¯ çˆ¬å–è¯é¢˜çš„ç²¾ååŒºå†…å®¹
    
    å‚æ•°:
        topicID: è¯é¢˜ID
        q_list: é—®é¢˜åˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨çˆ¬å–ç»“æœ
    """
    # Selected posts ç²¾å
    url = (
        "https://www.zhihu.com/api/v5.1/topics/"
        + topicID
        + "/feeds/top_activity?offset=0&limit=50"
    )
    while url:
        try:
            text = get_url_text(url)
            if text:  # ç¡®ä¿è·å–åˆ°å†…å®¹æ‰è§£æ
                url = parseJson(text, q_list)
            else:
                break
        except:
            print(f"ç›®å‰å·²æœ‰{len(q_list)}æ¡æ•°æ®")
            break
    print("crawl_ç²¾å: å®Œæˆ")

def crawl_3(topicID, q_list):
    """
    ğŸ¯ çˆ¬å–è¯é¢˜çš„ç­‰å¾…å›ç­”åŒºå†…å®¹
    
    å‚æ•°:
        topicID: è¯é¢˜ID
        q_list: é—®é¢˜åˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨çˆ¬å–ç»“æœ
    """
    # Awaiting answers ç­‰å¾…å›ç­”
    url = (
        "https://www.zhihu.com/api/v5.1/topics/"
        + topicID
        + "/feeds/top_question?offset=0&limit=50"
    )
    while url:
        try:
            text = get_url_text(url)
            if text:  # ç¡®ä¿è·å–åˆ°å†…å®¹æ‰è§£æ
                url = parseJson(text, q_list)
            else:
                break
        except:
            print(f"ç›®å‰å·²æœ‰{len(q_list)}æ¡æ•°æ®")
            break

    url = (
        "https://www.zhihu.com/api/v5.1/topics/"
        + topicID
        + "/feeds/new_question?offset=0&limit=50"
    )
    while url:
        try:
            text = get_url_text(url)
            if text:  # ç¡®ä¿è·å–åˆ°å†…å®¹æ‰è§£æ
                url = parseJson(text, q_list)
            else:
                break
        except:
            print(f"ç›®å‰å·²æœ‰{len(q_list)}æ¡æ•°æ®")
            break

    print("crawl_ç­‰å¾…å›ç­”: å®Œæˆ")

if __name__ == "__main__":
    # æ¼©æ¶¡é¸£äºº: 20204759
    # æ˜¥é‡æ¨±: 20135411
    #TODO æŒ‡å®šè¦çˆ¬å–çš„è¯é¢˜ID
    topicID_list = ["19556554"]
    q_list = []

    print(f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    print(f"è„šæœ¬æ‰€åœ¨ç›®å½•: {os.path.dirname(os.path.abspath(__file__))}")

    for topicID in topicID_list:
        print(f"ğŸš€ å¼€å§‹çˆ¬å–è¯é¢˜ID: {topicID}")
        
        # ä¼ é€’q_listå‚æ•°ç»™æ‰€æœ‰çˆ¬å–å‡½æ•°
        crawl_1(topicID, q_list)
        crawl_2(topicID, q_list)
        crawl_3(topicID, q_list)
        
        print(f"ğŸ“Š æ€»å…±çˆ¬å–åˆ° {len(q_list)} æ¡æ•°æ®")
        
        # ä¿å­˜æ•°æ®åˆ°CSVæ–‡ä»¶
        save_data(q_list, 'data/question_list.csv')
