from .db_util import execute_query,write_to_article
from ..settings import settings
from selenium import webdriver
from selenium.webdriver.common.by import By
import time
import psycopg2
from psycopg2 import Error
# ç»“åˆ Selenium å’Œ Trafilatura çš„å®Œæ•´è§£å†³æ–¹æ¡ˆ
import time
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from openai import OpenAI

def selenium_trafilatura_extract(url, driver=None, wait_time=3, save_html=False):
    """
    ä½¿ç”¨ Selenium è·å–ç½‘é¡µ HTMLï¼Œç„¶åç”¨ Trafilatura æå–ä¸­æ–‡å†…å®¹
    
    Args:
        url (str): ç›®æ ‡ç½‘é¡µURL
        driver: Selenium WebDriverå¯¹è±¡ï¼Œå¦‚æœä¸ºNoneä¼šåˆ›å»ºæ–°çš„
        wait_time (int): é¡µé¢åŠ è½½ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
        save_html (bool): æ˜¯å¦ä¿å­˜HTMLæ–‡ä»¶åˆ°æœ¬åœ°
    
    Returns:
        dict: åŒ…å«æå–å†…å®¹å’Œå…ƒæ•°æ®çš„å­—å…¸
    """
    
    # æ£€æŸ¥æ˜¯å¦å®‰è£…äº†trafilatura
    try:
        import trafilatura
    except ImportError:
        return {
            'success': False,
            'error': 'trafilaturaæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install trafilatura',
            'url': url
        }
    
    # åˆ›å»ºæˆ–ä½¿ç”¨ç°æœ‰çš„driver
    driver_created = False
    if driver is None:
        try:
            from selenium import webdriver
            options = webdriver.ChromeOptions()
            options.add_argument('--headless')  # æ— å¤´æ¨¡å¼ï¼Œæé«˜æ•ˆç‡
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-gpu')
            options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
            
            driver = webdriver.Chrome(options=options)
            driver_created = True
            print(f"âœ… åˆ›å»ºäº†æ–°çš„Chrome Driver")
        except Exception as e:
            return {
                'success': False,
                'error': f'æ— æ³•åˆ›å»ºWebDriver: {str(e)}',
                'url': url
            }
    
    try:
        print(f"ğŸŒ æ­£åœ¨è®¿é—®: {url}")
        
        # è®¿é—®ç½‘é¡µ
        driver.get(url)
        
        # ç­‰å¾…é¡µé¢åŠ è½½
        time.sleep(wait_time)
        
        # ç­‰å¾…bodyå…ƒç´ åŠ è½½å®Œæˆ
        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
        except:
            print("âš ï¸ é¡µé¢åŠ è½½å¯èƒ½ä¸å®Œæ•´ï¼Œç»§ç»­å¤„ç†...")
        
        # è·å–é¡µé¢æ ‡é¢˜
        page_title = driver.title
        print(f"ğŸ“„ é¡µé¢æ ‡é¢˜: {page_title}")
        
        # è·å–å®Œæ•´çš„HTMLæºç 
        html_content = driver.page_source
        print(f"ğŸ“ HTMLé•¿åº¦: {len(html_content)} å­—ç¬¦")
        
        # ä¿å­˜HTMLæ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
        html_file_path = None
        if save_html:
            from urllib.parse import urlparse
            import re
            
            # ä»URLç”Ÿæˆæ–‡ä»¶å
            parsed_url = urlparse(url)
            filename = re.sub(r'[^a-zA-Z0-9\u4e00-\u9fff]', '_', parsed_url.netloc + parsed_url.path)
            html_file_path = f"{filename[:50]}.html"
            
            try:
                with open(html_file_path, 'w', encoding='utf-8') as f:
                    f.write(html_content)
                print(f"ğŸ’¾ HTMLå·²ä¿å­˜åˆ°: {html_file_path}")
            except Exception as e:
                print(f"âš ï¸ HTMLä¿å­˜å¤±è´¥: {e}")
        
        # ä½¿ç”¨ Trafilatura æå–å†…å®¹
        print("ğŸ” ä½¿ç”¨ Trafilatura æå–å†…å®¹...")
        
        # åŸºç¡€æ–‡æœ¬æå–
        extracted_text = trafilatura.extract(html_content)
        
        # æå–å…ƒæ•°æ®
        metadata = trafilatura.extract_metadata(html_content)
        
        # é«˜çº§æå–ï¼ˆåŒ…å«æ›´å¤šé€‰é¡¹ï¼‰
        advanced_text = trafilatura.extract(
            html_content,
            favor_precision=True,    # æé«˜ç²¾ç¡®åº¦
            favor_recall=False,      # é™ä½å¬å›ç‡ï¼Œæé«˜è´¨é‡
            include_comments=False,  # ä¸åŒ…å«è¯„è®º
            include_tables=True,     # åŒ…å«è¡¨æ ¼
            include_formatting=True, # ä¿ç•™åŸºæœ¬æ ¼å¼
            include_links=False,     # ä¸åŒ…å«é“¾æ¥
            include_images=False     # ä¸åŒ…å«å›¾ç‰‡
        )
        
        # æå–ç»“æ„åŒ–æ•°æ®
        structured_data = trafilatura.extract(
            html_content,
            output_format='json',    # JSONæ ¼å¼è¾“å‡º
            include_formatting=True
        )
        
        # ç»„ç»‡è¿”å›ç»“æœ
        result = {
            'success': True,
            'url': url,
            'page_title': page_title,
            'selenium_title': page_title,
            'extracted_title': metadata.title if metadata else page_title,
            'basic_text': extracted_text,
            'advanced_text': advanced_text,
            'structured_data': structured_data,
            'metadata': {
                'title': metadata.title if metadata else None,
                'author': metadata.author if metadata else None,
                'date': metadata.date if metadata else None,
                'description': metadata.description if metadata else None,
                'sitename': metadata.sitename if metadata else None,
                'language': metadata.language if metadata else None,
                'url': metadata.url if metadata else url
            },
            'statistics': {
                'html_length': len(html_content),
                'basic_text_length': len(extracted_text) if extracted_text else 0,
                'advanced_text_length': len(advanced_text) if advanced_text else 0,
                'has_content': bool(extracted_text and len(extracted_text) > 50)
            },
            'html_file_path': html_file_path,
            'processing_time': time.time()
        }
        
        # è¾“å‡ºæå–ç»“æœæ‘˜è¦
        if extracted_text:
            print(f"âœ… å†…å®¹æå–æˆåŠŸ!")
            print(f"   åŸºç¡€æå–: {len(extracted_text)} å­—ç¬¦")
            print(f"   é«˜çº§æå–: {len(advanced_text) if advanced_text else 0} å­—ç¬¦")
            print(f"   æå–æ ‡é¢˜: {result['extracted_title']}")
            print(f"   å†…å®¹é¢„è§ˆ: {extracted_text[:100]}...")
        else:
            print(f"âŒ å†…å®¹æå–å¤±è´¥æˆ–å†…å®¹ä¸ºç©º")
            result['success'] = False
            result['error'] = 'æå–çš„å†…å®¹ä¸ºç©º'
        
        return result
        
    except Exception as e:
        error_msg = f"å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}"
        print(f"âŒ {error_msg}")
        return {
            'success': False,
            'error': error_msg,
            'url': url
        }
    
    finally:
        # å¦‚æœæ˜¯åˆ›å»ºçš„æ–°driverï¼Œåˆ™å…³é—­å®ƒ
        if driver_created and driver:
            try:
                driver.quit()
                print("ğŸšª å·²å…³é—­WebDriver")
            except:
                pass

# æ‰¹é‡å¤„ç†å‡½æ•°
def batch_selenium_trafilatura_extract(urls, driver=None, wait_time=3, max_urls=5):
    """
    æ‰¹é‡å¤„ç†å¤šä¸ªURL
    
    Args:
        urls (list): URLåˆ—è¡¨
        driver: å…±äº«çš„WebDriverå¯¹è±¡
        wait_time (int): æ¯ä¸ªé¡µé¢çš„ç­‰å¾…æ—¶é—´
        max_urls (int): æœ€å¤§å¤„ç†URLæ•°é‡
    
    Returns:
        list: å¤„ç†ç»“æœåˆ—è¡¨
    """
    
    results = []
    urls_to_process = urls[:max_urls]
    
    print(f"ğŸš€ å¼€å§‹æ‰¹é‡å¤„ç† {len(urls_to_process)} ä¸ªURL")
    print("=" * 60)
    
    # å¦‚æœæ²¡æœ‰æä¾›driverï¼Œåˆ›å»ºä¸€ä¸ªå…±äº«çš„
    driver_created = False
    if driver is None:
        try:
            from selenium import webdriver
            options = webdriver.ChromeOptions()
            options.add_argument('--headless')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            
            driver = webdriver.Chrome(options=options)
            driver_created = True
            print("âœ… åˆ›å»ºå…±äº«WebDriver")
        except Exception as e:
            print(f"âŒ æ— æ³•åˆ›å»ºWebDriver: {e}")
            return []
    
    try:
        for i, url in enumerate(urls_to_process, 1):
            print(f"\\nğŸ“ å¤„ç†ç¬¬ {i}/{len(urls_to_process)} ä¸ªURL")
            print("-" * 40)
            
            result = selenium_trafilatura_extract(
                url, 
                driver=driver, 
                wait_time=wait_time, 
                save_html=False
            )
            
            results.append(result)
            
            # é¿å…è¿‡å¿«è¯·æ±‚
            if i < len(urls_to_process):
                time.sleep(1)
    
    finally:
        if driver_created and driver:
            driver.quit()
            print("\\nğŸšª å·²å…³é—­å…±äº«WebDriver")
    
    # ç»Ÿè®¡ç»“æœ
    successful = sum(1 for r in results if r.get('success', False))
    print(f"\\nğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆ: {successful}/{len(urls_to_process)} æˆåŠŸ")
    
    return results


def get_baidu_hot_news():
    # ä¼˜åŒ–Chromeå¯åŠ¨é…ç½®ï¼Œæé«˜å¯åŠ¨é€Ÿåº¦
    options = webdriver.ChromeOptions()
    # åŸºç¡€æ€§èƒ½ä¼˜åŒ–é€‰é¡¹
    options.add_argument('--headless') #ä¸éœ€è¦ç•Œé¢
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument('--disable-gpu')
    options.add_argument('--disable-web-security')
    options.add_argument('--disable-features=VizDisplayCompositor')
    # ç½‘ç»œå’Œæ›´æ–°ç›¸å…³ä¼˜åŒ–
    options.add_argument('--disable-background-networking')
    options.add_argument('--disable-background-timer-throttling')
    options.add_argument('--disable-backgrounding-occluded-windows')
    options.add_argument('--disable-component-update')
    options.add_argument('--disable-default-apps')
    options.add_argument('--disable-extensions')
    # æ—¥å¿—å’Œå´©æºƒæŠ¥å‘Šä¼˜åŒ–
    options.add_argument('--disable-logging')
    options.add_argument('--disable-crash-reporter')
    # å¦‚æœä¸éœ€è¦ç•Œé¢å¯ä»¥å¯ç”¨æ— å¤´æ¨¡å¼ï¼ˆä¼šæ›´å¿«ï¼‰
    # options.add_argument('--headless')

    with open('selenium_log.txt', 'a', encoding='utf-8') as f:
        f.write("ğŸš€ æ­£åœ¨å¯åŠ¨Chromeæµè§ˆå™¨...\n")
    start_time = time.time()
    driver = webdriver.Chrome(options=options)
    end_time = time.time()
    with open('selenium_log.txt', 'a', encoding='utf-8') as f:
        f.write(f"âœ… Chromeå¯åŠ¨å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f} ç§’\n")

    driver.implicitly_wait(0.5)
    #è¿›å…¥é¦–é¡µæ‰¾åˆ°çƒ­ç‚¹é¡µé¢
    #è¿›å…¥ç™¾åº¦é¡µé¢
    driver.get("https://www.baidu.com")
    # ä½¿ç”¨CSSé€‰æ‹©å™¨å®šä½aæ ‡ç­¾ï¼Œç„¶åè·å–hrefå±æ€§
    # æ ¹æ®å›¾ç‰‡ä¸­çš„HTMLç»“æ„ï¼Œaæ ‡ç­¾æœ‰class="title-content tag-width c-link c-font-medium c-line-clamp1"
    #urls = driver.find_elements(by=By.CSS_SELECTOR, value="a.title-content.tag-width.c-link.c-font-medium.c-line-clamp1")
    urls = driver.find_elements(by=By.CSS_SELECTOR, value="a.title-content.c-link.c-font-medium.c-line-clamp1")
    #urls =urls+urls2
    specific_news = {}
    #è·å–ç™¾åº¦é¦–é¡µçš„çƒ­ç‚¹urlåˆ—è¡¨
    hot_href_list = []  #æ¯ä¸€ä¸ªæ‰“å¼€éƒ½æ˜¯å…·ä½“çƒ­ç‚¹æ–°é—»é¡µé¢
    hot_text_list = []
    specific_news = {}
    for url in urls:
        text = url.text
        href = url.get_attribute('href')
        hot_href_list.append(href)
        hot_text_list.append(text)
    href_value_list=[]
    for url,hot_text in zip(hot_href_list,hot_text_list):
        #æ‰“å¼€çƒ­ç‚¹åˆ—è¡¨ä¸­çš„ä¸€ä¸ª
        driver.get(url)
        time.sleep(1)
        #è·å–"æ›´å¤šæ¶ˆæ¯"çš„url
        # è·å–é¡µé¢ç¬¬ä¸€ä¸ªh3æ ‡ç­¾ï¼ˆclass='t'ï¼‰ï¼Œç„¶åè·å–è¯¥h3æ ‡ç­¾ä¸­çš„aæ ‡ç­¾çš„hrefå€¼
        #è¿™æ˜¯æ™®é€šçš„æŸ¥çœ‹æ›´å¤šæ¶ˆæ¯url
        try:
                # ä½¿ç”¨CSSé€‰æ‹©å™¨å®šä½ç¬¬ä¸€ä¸ªclassä¸º't'çš„h3æ ‡ç­¾
            h3_element = driver.find_element(by=By.CSS_SELECTOR, value="h3.t")
            
            # åœ¨h3æ ‡ç­¾å†…æŸ¥æ‰¾aæ ‡ç­¾
            a_element = h3_element.find_element(by=By.TAG_NAME, value="a")
            
            # è·å–aæ ‡ç­¾çš„hrefå±æ€§å€¼
            href_value = a_element.get_attribute('href')
            # href_value_list.append(href_value)
            with open('selenium_log.txt', 'a', encoding='utf-8') as f:
                f.write(f'æ›´å¤šæ¶ˆæ¯ {href_value}\n')
        except Exception as e:
            with open('selenium_log.txt', 'a', encoding='utf-8') as f:
                f.write(f"è·å–å…ƒç´ æ—¶å‡ºé”™: {e}\n")
            href_value = None
        if href_value:
            driver.get(href_value)
            with open('selenium_log.txt', 'a', encoding='utf-8') as f:
                f.write(f'è¿›å…¥çƒ­ç‚¹{hot_text}é¡µé¢\n')
            title = driver.title
            with open('selenium_log.txt', 'a', encoding='utf-8') as f:
                f.write(f"æ›´å¤šæ¶ˆæ¯é¡µé¢æ ‡é¢˜: {title}\n")
            # è·å–é¡µé¢æ‰€æœ‰åŒ…å«aria-labelå±æ€§çš„å…ƒç´ 
            elements_with_aria_label = driver.find_elements(by=By.CSS_SELECTOR, value="[aria-label]")

            with open('selenium_log.txt', 'a', encoding='utf-8') as f:
                f.write(f"æ‰¾åˆ° {len(elements_with_aria_label)} ä¸ªåŒ…å«aria-labelå±æ€§çš„å…ƒç´ \n")
                f.write("=" * 80 + "\n")
            single_new_list=[]
            # éå†æ¯ä¸ªå…ƒç´ å¹¶æ‰“å°å…¶åŸå§‹HTMLä»£ç 
            for i, element in enumerate(elements_with_aria_label, 1):
                # try:
                #     # è·å–å…ƒç´ çš„aria-labelå±æ€§å€¼
                #     aria_label_value = element.get_attribute('aria-label')
                #     # è·å–å…ƒç´ çš„åŸå§‹HTMLä»£ç 
                #     html_code = element.get_attribute('outerHTML')
                    
                #     # å°†æ•°æ®å†™å…¥txtæ–‡ä»¶
                #     with open('aria_label_data.txt', 'a', encoding='utf-8') as f:
                #         f.write(f"å…ƒç´  {i}:\n")
                #         f.write(f"aria-labelå€¼: {aria_label_value}\n")
                #         f.write(f"HTMLä»£ç : {html_code}\n")
                #         f.write("-" * 50 + "\n")
                # except Exception as e:
                #     print(f"å¤„ç†ç¬¬ {i} ä¸ªå…ƒç´ æ—¶å‡ºé”™: {e}")
                #     print("-" * 50)    
                # æ£€æŸ¥å…ƒç´ ç»“æ„ï¼Œå¯»æ‰¾aæ ‡ç­¾-spanæ ‡ç­¾-spanæ ‡ç­¾-spanæ ‡ç­¾çš„æ¨¡å¼
                if i >= 2:  # ä»ç¬¬ä¸‰ä¸ªå…ƒç´ å¼€å§‹æ£€æŸ¥
                    # æ£€æŸ¥å½“å‰å…ƒç´ åŠå…¶å3ä¸ªå…ƒç´ æ˜¯å¦å½¢æˆa-span-span-spanæ¨¡å¼
                    if i + 3 <= len(elements_with_aria_label):  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„åç»­å…ƒç´ 
                        current_element = element
                        next_element_1 = elements_with_aria_label[i]  # å½“å‰å…ƒç´ 
                        next_element_2 = elements_with_aria_label[i+1]  # åä¸€ä¸ªå…ƒç´ 
                        next_element_3 = elements_with_aria_label[i+2]  # åä¸¤ä¸ªå…ƒç´ 
                        
                        # æ£€æŸ¥æ ‡ç­¾ç±»å‹
                        current_tag = current_element.tag_name
                        next_tag_1 = next_element_1.tag_name
                        next_tag_2 = next_element_2.tag_name
                        next_tag_3 = next_element_3.tag_name
                        
                        # æ£€æŸ¥æ˜¯å¦ç¬¦åˆ a-span-span-span çš„æ¨¡å¼
                        if (current_tag == 'a' and next_tag_1 == 'span' and 
                            next_tag_2 == 'span' and next_tag_3 == 'span'):
                            
                            try:
                                # è·å–ç¬¬ä¸€ä¸ªaæ ‡ç­¾çš„hrefå’Œtext
                                a_href = current_element.get_attribute('href')
                                a_text = current_element.text.strip()
                                
                                # è·å–ç¬¬äºŒä¸ªspanæ ‡ç­¾çš„aria-labelå€¼
                                second_span_aria_label = next_element_1.get_attribute('aria-label')
                                with open('selenium_log.txt', 'a', encoding='utf-8') as f:
                                    f.write(f"{second_span_aria_label}\n")
                                
                                # å°†ç¬¦åˆæ¡ä»¶çš„æ•°æ®å†™å…¥æ–‡ä»¶
                                with open('filtered_aria_label_data.txt', 'a', encoding='utf-8') as f:
                                    f.write(f"ç¬¦åˆæ¨¡å¼çš„å…ƒç´ ç»„ {i//4}:\n")
                                    f.write(f"aæ ‡ç­¾href: {a_href}\n")
                                    f.write(f"aæ ‡ç­¾text: {a_text}\n")
                                    f.write(f"ç¬¬äºŒä¸ªspançš„aria-label: {second_span_aria_label}\n")
                                    f.write("=" * 50 + "\n")
                                
                                # print(f"æ‰¾åˆ°ç¬¦åˆæ¨¡å¼çš„å…ƒç´ ç»„ {i//4}: aæ ‡ç­¾href={a_href}, text={a_text}, ç¬¬äºŒä¸ªspançš„aria-label={second_span_aria_label}")
                                if second_span_aria_label and (('å¤©' in second_span_aria_label) or ('å°æ—¶' in second_span_aria_label) or ('åˆ†é’Ÿ' in second_span_aria_label)):
                                    with open('selenium_log.txt', 'a', encoding='utf-8') as f:
                                        f.write('--åŠ å…¥åˆ—è¡¨--\n')
                                    single_new_list.append((href_value,a_text))
                            except Exception as e:
                                with open('selenium_log.txt', 'a', encoding='utf-8') as f:
                                    f.write(f"å¤„ç†ç¬¦åˆæ¨¡å¼çš„å…ƒç´ ç»„æ—¶å‡ºé”™: {e}\n")
        if len(single_new_list)>0:
                specific_news[hot_text]=single_new_list
        else:
            with open('selenium_log.txt', 'a', encoding='utf-8') as f:
                f.write(f'{hot_text}æŸ¥çœ‹æ›´å¤šæ¨¡å¼ä¸‹æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆå¾ªç¯æ¨¡å¼çš„æ–°é—»url\n')
        #å†æ¬¡æ‰“å¼€é¡µé¢ï¼Œè·å–æŸ¥çœ‹å®Œæ•´æ–°é—»æ¨¡å¼çš„url
        #æ‰“å¼€çƒ­ç‚¹åˆ—è¡¨ä¸­çš„ä¸€ä¸ª
        single_new_list=[]
        driver.get(url)
        time.sleep(1)
        try:
            # è·å–æ–°é¡µé¢ä¸­ç¬¬ä¸€ä¸ªæœ‰aria-labelå±æ€§ï¼Œä¸”å±æ€§å€¼ä¸­æœ‰'æŸ¥çœ‹å®Œæ•´'çš„aæ ‡ç­¾ä¸­ï¼Œå–å¾—aæ ‡ç­¾çš„hrefå€¼
            elements_with_aria_label = driver.find_elements(by=By.CSS_SELECTOR, value="[aria-label]")
            # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªåŒ…å«'æŸ¥çœ‹å®Œæ•´'çš„aæ ‡ç­¾
            for element in elements_with_aria_label:
                aria_label_value = element.get_attribute('aria-label')
                tag_name = element.tag_name
                # æ£€æŸ¥æ˜¯å¦æ˜¯aæ ‡ç­¾ä¸”aria-labelåŒ…å«'æŸ¥çœ‹å®Œæ•´'
                if tag_name == 'a' and aria_label_value and 'æŸ¥çœ‹å®Œæ•´' in aria_label_value:
                    href_value = element.get_attribute('href')
                    with open('selenium_log.txt', 'a', encoding='utf-8') as f:
                        f.write(f"æ‰¾åˆ°åŒ…å«'æŸ¥çœ‹å®Œæ•´'çš„aæ ‡ç­¾ï¼Œhref: {href_value}\n")
                    break
            with open('selenium_log.txt', 'a', encoding='utf-8') as f:
                f.write(f"æŸ¥çœ‹å®Œæ•´æ¨¡å¼ä¸‹æ‰¾åˆ°çš„æ–°é—»åˆ—è¡¨å…¥å£ï¼Œhref: {href_value}\n")
            if href_value:
                driver.get(href_value)
                time.sleep(1)
            # è·å–é¡µé¢æ‰€æœ‰classä¸­å«æœ‰contentçš„aæ ‡ç­¾ï¼ˆclassåŒ…å«contentä½†ä¸ä¸€å®šå®Œå…¨ç­‰äºcontentï¼‰
            elements_with_content_class = driver.find_elements(by=By.CSS_SELECTOR, value="a[class*='content']")
            for element in elements_with_content_class:
                href_value = element.get_attribute('href')
                text_content = element.text.strip()
                single_new_list.append((href_value,text_content))
                # class_value = element.get_attribute('class')
                # html_code = element.get_attribute('outerHTML')
                # with open('content_class_data.txt', 'a', encoding='utf-8') as f:
                #     f.write(f"å…ƒç´  {i}:\n")
            if len(single_new_list)>0:
                    if hot_text in specific_news:
                        specific_news[hot_text]=specific_news[hot_text]+single_new_list
                    else:
                        specific_news[hot_text]=single_new_list
                    with open('selenium_log.txt', 'a', encoding='utf-8') as f:
                        f.write(f'{hot_text}æŸ¥çœ‹å®Œæ•´æ¨¡å¼ä¸‹æ‰¾åˆ°ç¬¦åˆå¾ªç¯æ¨¡å¼çš„æ–°é—»urlï¼š\n{single_new_list}\n')
            else:
                with open('selenium_log.txt', 'a', encoding='utf-8') as f:
                    f.write(f'{hot_text}æŸ¥çœ‹æ›´å¤šæ¨¡å¼ä¸‹æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆå¾ªç¯æ¨¡å¼çš„æ–°é—»url\n')
        except Exception as e:
            with open('selenium_log.txt', 'a', encoding='utf-8') as f:
                f.write(f"å¤„ç†æŸ¥çœ‹å®Œæ•´æ¨¡å¼æ—¶å‡ºé”™: {e}\n")
    news_text = {}
    for key in specific_news.keys():
        print(f"=========å¤„ç†{key}çš„æ–°é—»======")
        url_list =[]
        info_list =[]
        for href,text in specific_news[key]:
            url_list.append(href)
        for url in url_list:
            try:
                res = selenium_trafilatura_extract(url=url,driver=driver)
                #é¡µé¢æ ‡é¢˜ã€æ–‡æœ¬ã€ä½œè€…ã€ç½‘å€ã€é“¾æ¥ã€æ—¥æœŸã€é•¿åº¦
                info_list.append((res['page_title'],res['advanced_text'],res['metadata']['author'],res['metadata']['sitename'],res['url'],res['metadata']['date'],res['statistics']['advanced_text_length']))
            except Exception as e:
                print(f" {e}")
        news_text[key] = info_list
    return news_text

def save_to_db(res,news_text):
    import datetime
    import json
    json_list =[json.loads(json_str.replace('```json\n', '').replace('\n```', '')) for json_str in res]
    # å‡†å¤‡æ’å…¥æ•°æ®çš„SQLè¯­å¥ (PostgreSQLä½¿ç”¨%så ä½ç¬¦)
    insert_sql = """
    INSERT INTO baidu_news (hottopic, page_title, content_text, author, site, url, update_time, content_length,absrtact,keywords)
    VALUES (%s, %s, %s, %s, %s, %s, %s, %s,%s,%s)
    """

    # éå†news_textå­—å…¸ï¼Œå°†æ•°æ®æ’å…¥æ•°æ®åº“
    i = 0
    for hottopic, news_list in news_text.items():     
        abstract = json_list[i].get('abstract', 'æ— æ•°æ®')
        keywords = json_list[i].get('keywords', 'æ— æ•°æ®')
        for news_item in news_list:
            try:
                # è§£åŒ…å…ƒç»„æ•°æ®
                page_title, content_text, author, site, url, update_time, content_length = news_item
                
                # å¤„ç†å¯èƒ½çš„Noneå€¼
                page_title = page_title if page_title else ''
                content_text = content_text if content_text else ''
                author = author if author else ''
                site = site if site else ''
                url = url if url else ''
                
                # ç‰¹æ®Šå¤„ç†æ—¥æœŸå­—æ®µ - ç¡®ä¿æ˜¯æœ‰æ•ˆçš„æ—¥æœŸæ ¼å¼æˆ–NULL
                if update_time and update_time.strip() and update_time != '':
                    # å¦‚æœæœ‰æœ‰æ•ˆçš„æ—¥æœŸï¼Œç›´æ¥ä½¿ç”¨
                    processed_update_time = update_time
                else:
                    # å¦‚æœæ—¥æœŸä¸ºç©ºæˆ–æ— æ•ˆï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ
                    processed_update_time = datetime.datetime.now().strftime('%Y-%m-%d')
                
                content_length = content_length if content_length else 0
                
                # æ‰§è¡Œæ’å…¥æ“ä½œ
                execute_query(insert_sql, (hottopic, page_title, content_text, author, site, url, processed_update_time, content_length,abstract,keywords))
                print(f"æˆåŠŸæ’å…¥: {hottopic} - {page_title}")
                
                
            except Exception as e:
                print(f"æ’å…¥å¤±è´¥ {hottopic}: {e}")
        i+=1

    print("æ•°æ®æ’å…¥å®Œæˆï¼")




def get_topics(news_text):
    topics_list = []
    # news_textçš„åˆ—è¡¨æœ‰å¯èƒ½æœ‰ç©ºå€¼
    for key in news_text.keys():
        info_list = news_text[key]
        #æ‹¼æ¥æ–‡æœ¬
        raw_texts = ''
        for i,info in enumerate(info_list):
            # æ£€æŸ¥info[1]æ˜¯å¦ä¸ºNoneï¼Œå¦‚æœæ˜¯åˆ™ä½¿ç”¨ç©ºå­—ç¬¦ä¸²
            content_text = info[1] if info[1] is not None else ''
            if content_text:  # åªæœ‰å½“å†…å®¹ä¸ä¸ºç©ºæ—¶æ‰æ·»åŠ 
                raw_texts += f'ç¬¬{i+1}ç¯‡ï¼š'+'\n'+content_text + '\n\n'
        #è·å–topics
        GET_TOPICS_PROMPT = """
        ä¸‹é¢æ˜¯å…³äºâ€œ{key}â€çš„ä¸€äº›æ–°é—»æŠ¥é“ï¼Œæ ¹æ®è¿™äº›æ–°é—»ï¼Œè¿›è¡Œæ–°é—»å†…å®¹æ€»ç»“ï¼Œæ€»ç»“å†…å®¹ä¸è¶…è¿‡50å­—ï¼Œå¹¶æå–å‡º5ä¸ªä¸»é¢˜å…³é”®è¯ã€‚
        è¿”å›jsonæ•°æ®ã€‚
        ç¤ºä¾‹è¿”å›å½¢å¼ï¼š{"abstract":"äººç¤¾éƒ¨å®£å¸ƒï¼Œè‡ªä»2025å¹´6æœˆèµ·ï¼Œä¸ªäººå…»è€é‡‘éœ€è¦ç¼´çº³3%çš„ä¸ªäººæ‰€å¾—ç¨",
        "keywords":"æ°‘ç”Ÿï¼Œç¨åŠ¡ï¼Œæ”¿åºœï¼Œå…»è€é‡‘ï¼Œä¸Šè°ƒç¨ç‡"}
        æ–°é—»æŠ¥é“æ˜¯ï¼š{raw_texts}ã€‚
        ä½ çš„å›ç­”ï¼š
        """
        GET_TOPICS_PROMPT = GET_TOPICS_PROMPT.replace('{key}',key)
        GET_TOPICS_PROMPT = GET_TOPICS_PROMPT.replace('{raw_texts}',raw_texts)
        #print('GET_TOPICS_PROMPT',GET_TOPICS_PROMPT)
        client = OpenAI(api_key=settings.DEEPSEEK_API, base_url="https://api.deepseek.com")
        response = client.chat.completions.create(
        model="deepseek-chat",
        messages=[
            {"role": "system", "content": "You are a helpful assistant"},
            {"role": "user", "content":GET_TOPICS_PROMPT },
        ],
        stream=False
        )
        system_msg = response.choices[0].message.content
        topics_list.append(system_msg)
    return topics_list
if __name__ == "__main__":
    news_text = get_baidu_hot_news()
    # è¿‡æ»¤æ‰ç©ºåˆ—è¡¨çš„æƒ…å†µ
    filtered_news_text = {key: value for key, value in news_text.items() if value}
    res = get_topics(news_text)
    save_to_db(res,news_text)

        
